# üîó Int√©gration de Vigostral-7B fine-tun√© avec Nanochat

Ce document explique comment int√©grer votre mod√®le Vigostral-7B-Chat fine-tun√© avec l'interface de chat de Nanochat.

## üéØ Objectif

Utiliser l'application compl√®te **nanochat** (interface web, CLI, outils comme calculatrice) mais avec votre mod√®le conversationnel fran√ßais personnalis√© au lieu du mod√®le anglais par d√©faut.

## ‚ö†Ô∏è Le D√©fi Technique

Il y a un probl√®me d'**incompatibilit√© d'architecture** :

| Aspect | Nanochat | Vigostral-7B-Chat |
|--------|----------|-------------------|
| **Architecture** | GPT custom (nanochat/gpt.py) | Mistral-7B (architecture Llama) |
| **Format checkpoints** | `.pt` PyTorch natif | HuggingFace format |
| **Tokenizer** | BPE custom (rustbpe) | Tokenizer Llama/Mistral |
| **Position encoding** | RoPE custom | RoPE standard |
| **Attention** | Multi-Query Attention (MQA) | Grouped-Query Attention (GQA) |

**Conclusion** : Nanochat ne peut PAS charger directement Vigostral car ce sont deux architectures compl√®tement diff√©rentes.

## üí° Solutions Possibles

### Option A : Cr√©er un wrapper/adapter pour nanochat

**Principe** : Modifier l'interface de nanochat pour qu'elle puisse charger des mod√®les HuggingFace comme Vigostral.

**Avantages** :
- ‚úÖ Garde l'interface utilisateur de nanochat (web, CLI)
- ‚úÖ Utilise votre mod√®le Vigostral fine-tun√©
- ‚úÖ Peut r√©utiliser les outils de nanochat (calculatrice, etc.)

**Inconv√©nients** :
- ‚ùå N√©cessite de modifier le code de nanochat
- ‚ùå Doit cr√©er une couche d'abstraction entre nanochat et HuggingFace
- ‚ùå Maintenance complexe si nanochat √©volue

**Fichiers √† modifier** :
```
nanochat/
‚îú‚îÄ‚îÄ checkpoint_manager.py  # Ajouter support HuggingFace
‚îú‚îÄ‚îÄ engine.py              # Adapter la g√©n√©ration pour Transformers
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ chat_cli.py        # D√©tecter type de mod√®le
    ‚îî‚îÄ‚îÄ chat_web.py        # Idem
```

**Exemple de code** (checkpoint_manager.py) :
```python
def load_model(source, device, phase, model_tag=None, step=None):
    if source.startswith("hf_"):
        # Charger depuis HuggingFace
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import PeftModel

        model_name = source.replace("hf_", "")
        base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

        # Si des adaptateurs LoRA existent, les charger
        if os.path.exists(f"./models/{model_name}_lora"):
            model = PeftModel.from_pretrained(base_model, f"./models/{model_name}_lora")
        else:
            model = base_model

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        return model, tokenizer, {}
    else:
        # Logique nanochat existante
        # ...
```

**Note** : Le nom correct du mod√®le Vigostral est `bofenghuang/vigostral-7b-chat` (pas `vigostral/vigostral-7b-chat`).

---

### Option B : Fork nanochat et cr√©er nanochat-french

**Principe** : Cr√©er une version standalone de nanochat qui utilise exclusivement des mod√®les HuggingFace fran√ßais.

**Avantages** :
- ‚úÖ Code clean et d√©di√© aux mod√®les fran√ßais
- ‚úÖ Peut simplifier beaucoup le code (pas besoin de GPT custom)
- ‚úÖ Facile √† maintenir ind√©pendamment

**Inconv√©nients** :
- ‚ùå Perd les synchronisations avec nanochat upstream
- ‚ùå Doit r√©impl√©menter certaines fonctionnalit√©s

**Structure propos√©e** :
```
nanochat-french/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ vigostral-finetuned/  # Votre mod√®le
‚îú‚îÄ‚îÄ chat_cli.py                # CLI simplifi√© pour Vigostral
‚îú‚îÄ‚îÄ chat_web.py                # Interface web
‚îî‚îÄ‚îÄ engine.py                  # Wrapper Transformers
```

---

### Option C : Interface web standalone avec Gradio

**Principe** : Cr√©er une interface web simple avec Gradio, sans r√©utiliser nanochat.

**Avantages** :
- ‚úÖ Tr√®s simple et rapide √† impl√©menter
- ‚úÖ Interface moderne et jolie
- ‚úÖ Aucune d√©pendance √† nanochat

**Inconv√©nients** :
- ‚ùå Perd les outils sp√©cifiques de nanochat (calculatrice, etc.)
- ‚ùå Moins de contr√¥le sur l'interface

**Code complet** (~50 lignes) :
```python
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Charger le mod√®le
model_name = "vigostral/vigostral-7b-chat"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
model = PeftModel.from_pretrained(model, "./vigostral-finetuned-final")
tokenizer = AutoTokenizer.from_pretrained(model_name)

def chat(message, history):
    # Formater le prompt
    prompt = f"<s>[INST] {message} [/INST]"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # G√©n√©rer
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("[/INST]")[1].strip()

    return response

# Interface Gradio
demo = gr.ChatInterface(
    chat,
    title="üá´üá∑ Chat Fran√ßais Personnalis√©",
    description="Mod√®le Vigostral-7B fine-tun√© sur votre style",
)

demo.launch(share=True)
```

---

## üéØ Recommandation : Option C (Gradio)

Pour commencer, je recommande **l'Option C** car :

1. **Simple** : 50 lignes de code, fonctionne imm√©diatement
2. **Pas de modifications complexes** : Pas besoin de forker nanochat
3. **Interface moderne** : Gradio est tr√®s joli et facile √† utiliser
4. **Partage facile** : `share=True` g√©n√®re un URL public temporaire

### √âtapes pour impl√©menter l'Option C

1. **Fine-tuner Vigostral sur Colab** (notebook fourni : `vigostral_finetune_colab.ipynb`)
2. **T√©l√©charger le mod√®le fine-tun√©** (~100-200 MB d'adaptateurs LoRA)
3. **Cr√©er l'interface Gradio localement** (voir code ci-dessus)
4. **Lancer** : `python chat_gradio.py`

---

## üìä Comparaison des Options

| Crit√®re | Option A (Wrapper) | Option B (Fork) | Option C (Gradio) |
|---------|-------------------|-----------------|-------------------|
| **Complexit√©** | üî¥ √âlev√©e | üü° Moyenne | üü¢ Faible |
| **Temps d'impl√©mentation** | 2-3 jours | 1-2 jours | 1-2 heures |
| **Interface** | Interface nanochat | Interface nanochat | Interface Gradio |
| **Outils (calculatrice)** | ‚úÖ Oui | ‚ö†Ô∏è √Ä r√©impl√©menter | ‚ùå Non (facilement ajoutable) |
| **Maintenance** | üî¥ Difficile | üü° Moyenne | üü¢ Simple |
| **Compatibilit√© future** | ‚ö†Ô∏è D√©pend de nanochat | ‚úÖ Ind√©pendant | ‚úÖ Ind√©pendant |

---

## üöÄ Prochaines √âtapes

### Phase 1 : Fine-tuning (Maintenant)
1. ‚úÖ Utiliser `vigostral_finetune_colab.ipynb` sur Google Colab
2. ‚úÖ Fine-tuner Vigostral-7B sur vos 123 dialogues avec LoRA
3. ‚úÖ T√©l√©charger le mod√®le fine-tun√©

### Phase 2 : Interface locale (Apr√®s fine-tuning)
1. Installer les d√©pendances localement :
   ```bash
   pip install transformers peft accelerate bitsandbytes gradio
   ```

2. Cr√©er `chat_gradio.py` avec le code fourni ci-dessus

3. Lancer l'interface :
   ```bash
   python chat_gradio.py
   ```

### Phase 3 : Am√©liorations (Optionnel)
- Ajouter des outils (calculatrice, recherche web)
- Cr√©er une vraie int√©gration avec nanochat (Option A ou B)
- D√©ployer en ligne (HuggingFace Spaces gratuit)

---

## üí° Notes Techniques Importantes

### Chargement du mod√®le en local

Si vous avez une **machine avec GPU** (CUDA) :
```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
```

Si vous √™tes sur **CPU/Mac** :
```python
# Utiliser quantization 4-bit pour r√©duire la m√©moire
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float32,  # CPU = float32
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="cpu"
)
```

**Attention** : Sur CPU, la g√©n√©ration sera **lente** (~1-2 tokens/seconde pour 7B params).

### Taille du mod√®le

- **Mod√®le de base (Vigostral-7B)** : ~14 GB (FP16) ou ~4 GB (4-bit)
- **Adaptateurs LoRA fine-tun√©s** : ~100-200 MB
- **Total avec LoRA** : ~14.2 GB (FP16) ou ~4.2 GB (4-bit)

---

## üìö Ressources Compl√©mentaires

- [Documentation Gradio](https://gradio.app/docs/)
- [Documentation PEFT (LoRA)](https://huggingface.co/docs/peft)
- [Vigostral-7B-Chat sur HuggingFace](https://huggingface.co/vigostral/vigostral-7b-chat)
- [Tutoriel LoRA officiel](https://huggingface.co/blog/lora)

---

## ‚ùì FAQ

### Q : Puis-je utiliser Vigostral fine-tun√© avec nanochat sans modifications ?
**R** : Non, nanochat utilise une architecture GPT custom incompatible avec Vigostral (architecture Mistral). Il faut soit cr√©er un wrapper (Option A), soit utiliser une interface standalone (Option C).

### Q : Est-ce que LoRA r√©duit la qualit√© du mod√®le ?
**R** : Non ! Des √©tudes montrent que LoRA donne des r√©sultats comparables au fine-tuning complet, tout en n'entra√Ænant que ~1% des param√®tres.

### Q : Combien de temps prend le fine-tuning sur Colab ?
**R** : Environ **20-30 minutes** sur un GPU T4 gratuit pour 123 dialogues avec 3 epochs.

### Q : Puis-je fine-tuner sur plus de dialogues ?
**R** : Oui ! Plus vous avez de dialogues de qualit√© (200-500+), meilleur sera le r√©sultat. Le temps d'entra√Ænement augmente proportionnellement.

### Q : Le mod√®le fine-tun√© fonctionne-t-il sur Mac/CPU ?
**R** : Oui, mais ce sera **tr√®s lent** (~1-2 tokens/seconde). Pour une exp√©rience fluide, utilisez un GPU (T4, RTX 3060, M1/M2 Mac avec MPS).

---

**Made with ‚ù§Ô∏è for the French AI community**

Pour toute question : [Issues GitHub](https://github.com/cladjidane/nanochat-french-tutorial/issues)
