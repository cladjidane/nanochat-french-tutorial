"""
Interface de chat Gradio pour Vigostral-7B-Chat fine-tun√©.

Installation :
    pip install transformers peft accelerate bitsandbytes gradio torch

Usage :
    python chat_gradio.py

    Puis ouvrez votre navigateur sur l'URL affich√©e (g√©n√©ralement http://127.0.0.1:7860)
"""

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import os

# Configuration
MODEL_NAME = "bofenghuang/vigostral-7b-chat"
LORA_ADAPTER_PATH = "./vigostral-finetuned-final"  # Chemin vers vos adaptateurs LoRA

# D√©tection du device
if torch.cuda.is_available():
    DEVICE = "cuda"
    print(f"‚úÖ GPU d√©tect√© : {torch.cuda.get_device_name(0)}")
elif torch.backends.mps.is_available():
    DEVICE = "mps"
    print("‚úÖ Apple Silicon (MPS) d√©tect√©")
else:
    DEVICE = "cpu"
    print("‚ö†Ô∏è  Aucun GPU d√©tect√©, utilisation du CPU (sera lent)")


def load_model():
    """
    Charge le mod√®le Vigostral-7B-Chat avec les adaptateurs LoRA fine-tun√©s.
    """
    print(f"\nüì• Chargement du mod√®le {MODEL_NAME}...")

    # Configuration de quantization 4-bit pour √©conomiser la m√©moire
    if DEVICE == "cuda":
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
    else:
        # CPU/MPS : pas de quantization 4-bit (pas support√©)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32 if DEVICE == "cpu" else torch.float16,
            device_map=DEVICE,
            trust_remote_code=True,
        )

    # Charger le tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Charger les adaptateurs LoRA si disponibles
    if os.path.exists(LORA_ADAPTER_PATH):
        print(f"üì• Chargement des adaptateurs LoRA depuis {LORA_ADAPTER_PATH}...")
        model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)
        print("‚úÖ Adaptateurs LoRA charg√©s (mod√®le fine-tun√©)")
    else:
        print(f"‚ö†Ô∏è  Adaptateurs LoRA non trouv√©s dans {LORA_ADAPTER_PATH}")
        print("   ‚Üí Utilisation du mod√®le de base (non fine-tun√©)")

    print(f"‚úÖ Mod√®le charg√© avec succ√®s !")
    print(f"   Taille en m√©moire : {model.get_memory_footprint() / 1e9:.2f} GB\n")

    return model, tokenizer


# Charger le mod√®le au d√©marrage
MODEL, TOKENIZER = load_model()


def generate_response(message, history, temperature=0.7, max_tokens=200, top_p=0.9):
    """
    G√©n√®re une r√©ponse du mod√®le pour un message utilisateur.

    Args:
        message (str): Message de l'utilisateur
        history (list): Historique de la conversation (non utilis√© pour l'instant)
        temperature (float): Contr√¥le la cr√©ativit√© (0.1 = tr√®s d√©terministe, 1.0 = tr√®s cr√©atif)
        max_tokens (int): Nombre maximum de tokens √† g√©n√©rer
        top_p (float): Nucleus sampling (0.9 recommand√©)

    Returns:
        str: R√©ponse g√©n√©r√©e par le mod√®le
    """
    # Formater le prompt au format Vigostral (Llama-2 style)
    prompt = f"<s>[INST] {message} [/INST]"

    # Tokenizer
    inputs = TOKENIZER(prompt, return_tensors="pt").to(MODEL.device)

    # G√©n√©rer
    with torch.no_grad():
        outputs = MODEL.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=TOKENIZER.eos_token_id,
        )

    # D√©coder la r√©ponse
    full_response = TOKENIZER.decode(outputs[0], skip_special_tokens=True)

    # Extraire seulement la r√©ponse (apr√®s [/INST])
    if "[/INST]" in full_response:
        response = full_response.split("[/INST]")[1].strip()
    else:
        response = full_response

    return response


# Interface Gradio
with gr.Blocks(title="üá´üá∑ Chat Fran√ßais Personnalis√©") as demo:
    gr.Markdown(
        """
        # üá´üá∑ Chat Fran√ßais Personnalis√©

        Propuls√© par **Vigostral-7B-Chat** fine-tun√© sur votre style personnel.

        ---
        """
    )

    chatbot = gr.Chatbot(
        label="Conversation",
        height=500,
        show_label=True,
        avatar_images=(None, "ü§ñ"),
    )

    with gr.Row():
        msg = gr.Textbox(
            label="Votre message",
            placeholder="Tapez votre message ici...",
            show_label=False,
            scale=9,
        )
        send_btn = gr.Button("Envoyer", variant="primary", scale=1)

    with gr.Accordion("‚öôÔ∏è Param√®tres avanc√©s", open=False):
        temperature = gr.Slider(
            minimum=0.1,
            maximum=1.5,
            value=0.7,
            step=0.1,
            label="Temperature (cr√©ativit√©)",
            info="Plus haut = plus cr√©atif, plus bas = plus d√©terministe",
        )
        max_tokens = gr.Slider(
            minimum=50,
            maximum=500,
            value=200,
            step=10,
            label="Nombre maximum de tokens",
            info="Longueur maximale de la r√©ponse",
        )
        top_p = gr.Slider(
            minimum=0.1,
            maximum=1.0,
            value=0.9,
            step=0.05,
            label="Top-p (nucleus sampling)",
            info="Diversit√© des r√©ponses",
        )

    clear_btn = gr.Button("üóëÔ∏è Effacer la conversation")

    def user_message(user_msg, history):
        """Ajoute le message utilisateur √† l'historique."""
        return "", history + [[user_msg, None]]

    def bot_response(history, temp, max_tok, top_p_val):
        """G√©n√®re et ajoute la r√©ponse du bot √† l'historique."""
        user_msg = history[-1][0]
        bot_msg = generate_response(
            user_msg,
            history[:-1],
            temperature=temp,
            max_tokens=max_tok,
            top_p=top_p_val,
        )
        history[-1][1] = bot_msg
        return history

    # √âv√©nements
    msg.submit(user_message, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot_response, [chatbot, temperature, max_tokens, top_p], chatbot
    )
    send_btn.click(user_message, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot_response, [chatbot, temperature, max_tokens, top_p], chatbot
    )
    clear_btn.click(lambda: None, None, chatbot, queue=False)

    gr.Markdown(
        """
        ---

        ### üìù Notes

        - **Temperature** : Contr√¥le la cr√©ativit√© du mod√®le (0.7 recommand√©)
        - **Max tokens** : Longueur maximale de la r√©ponse
        - **Top-p** : Contr√¥le la diversit√© (0.9 recommand√©)

        ### üõ†Ô∏è Informations techniques

        - Mod√®le : Vigostral-7B-Chat (7 milliards de param√®tres)
        - Architecture : Mistral-7B
        - Fine-tuning : LoRA (Low-Rank Adaptation)
        - Device : """ + DEVICE + """

        ---

        **Cr√©√© avec ‚ù§Ô∏è pour la communaut√© IA fran√ßaise**
        """
    )

if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("üöÄ Lancement de l'interface Gradio...")
    print("=" * 80 + "\n")

    demo.launch(
        share=False,  # Mettez True pour g√©n√©rer un lien public temporaire
        server_name="0.0.0.0",  # Accessible depuis le r√©seau local
        server_port=7860,
    )
