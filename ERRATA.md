# üîß Errata & Corrections

## ‚úÖ Correction du nom du mod√®le Vigostral (2025-10-17)

### Probl√®me

Dans la version initiale du projet, le nom du mod√®le HuggingFace √©tait incorrect :
- ‚ùå **Incorrect** : `vigostral/vigostral-7b-chat`
- ‚úÖ **Correct** : `bofenghuang/vigostral-7b-chat`

### Erreur rencontr√©e

Si vous utilisez l'ancien nom, vous obtiendrez cette erreur :

```
RepositoryNotFoundError: 404 Client Error.
Repository Not Found for url: https://huggingface.co/vigostral/vigostral-7b-chat/resolve/main/config.json.
```

### Fichiers corrig√©s

Les fichiers suivants ont √©t√© mis √† jour avec le bon nom de mod√®le :

1. ‚úÖ **`vigostral_finetune_colab.ipynb`**
   - Ligne : `model_name = "bofenghuang/vigostral-7b-chat"`
   - Cellule de test final √©galement corrig√©e

2. ‚úÖ **`chat_gradio.py`**
   - Ligne 20 : `MODEL_NAME = "bofenghuang/vigostral-7b-chat"`

3. ‚úÖ **`INTEGRATION_NANOCHAT.md`**
   - Ajout d'une note explicative

### Solution si vous avez d√©j√† commenc√©

Si vous avez d√©j√† upload√© le notebook sur Colab avec l'ancien nom :

**Option 1** : Re-t√©l√©chargez le notebook depuis GitHub
1. Allez sur https://github.com/cladjidane/nanochat-french-tutorial
2. T√©l√©chargez le nouveau `vigostral_finetune_colab.ipynb`
3. Uploadez-le √† nouveau sur Colab

**Option 2** : Modifiez manuellement dans Colab
1. Dans Colab, trouvez la cellule avec `model_name = "vigostral/vigostral-7b-chat"`
2. Changez en : `model_name = "bofenghuang/vigostral-7b-chat"`
3. R√©-ex√©cutez la cellule

### Informations sur le mod√®le

**Mod√®le Vigostral-7B-Chat**
- **Cr√©ateur** : Bo Feng Huang (bofenghuang)
- **Organisation** : Vigogne AI
- **URL HuggingFace** : https://huggingface.co/bofenghuang/vigostral-7b-chat
- **Licence** : Apache 2.0
- **Base** : Mistral-7B-v0.1
- **Donn√©es d'entra√Ænement** : ~213k dialogues fran√ßais (distill√©s de GPT-3.5/4)

### Formats disponibles

Le mod√®le Vigostral-7B-Chat est disponible en plusieurs formats :

- **Standard** : `bofenghuang/vigostral-7b-chat` (utilis√© dans ce projet)
- **GGUF** : `FlorianJc/Vigostral-7b-Chat-GGUF` (pour llama.cpp)
- **GGUF** : `TheBloke/Vigostral-7B-Chat-GGUF` (alternative)
- **AWQ** : Versions quantiz√©es pour inference rapide
- **GPTQ** : Versions quantiz√©es alternatives

Pour ce projet, nous utilisons la **version standard** car elle fonctionne parfaitement avec Transformers + PEFT (LoRA).

---

**Date de correction** : 2025-10-17
**Signal√© par** : fabiencanu (cladjidane)
