# üá´üá∑ Fine-tuning d'un Mod√®le Conversationnel Fran√ßais

Tutorial complet pour fine-tuner **Vigostral-7B-Chat**, un mod√®le conversationnel fran√ßais de 7 milliards de param√®tres, sur vos propres dialogues avec **Google Colab** (GPU gratuit) et **LoRA** (Low-Rank Adaptation).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cladjidane/nanochat-french-tutorial/blob/main/vigostral_finetune_colab.ipynb)

## üéØ Objectif

Cr√©er un chatbot en fran√ßais qui parle dans **votre style personnel** en fine-tunant Vigostral-7B-Chat sur vos conversations.

**Ce que vous obtiendrez** :
- ‚úÖ Un mod√®le conversationnel fran√ßais performant
- ‚úÖ Fine-tun√© sur vos dialogues personnels (votre style, votre vocabulaire)
- ‚úÖ Utilisable via une interface web moderne (Gradio)
- ‚úÖ Entra√Ænement rapide (~20-30 minutes sur GPU T4 gratuit)
- ‚úÖ Fonctionne localement apr√®s t√©l√©chargement

## üöÄ D√©marrage Rapide

### √âtape 1 : Fine-tuning sur Google Colab (20-30 minutes)

1. Cliquez sur le badge "Open in Colab" ci-dessus
2. **Runtime** ‚Üí **Change runtime type** ‚Üí S√©lectionnez **T4 GPU**
3. Uploadez votre dataset `combined_dataset.jsonl` quand demand√©
4. Ex√©cutez toutes les cellules du notebook
5. T√©l√©chargez le mod√®le fine-tun√© √† la fin

### √âtape 2 : Interface locale (5 minutes)

1. Installez les d√©pendances :
   ```bash
   pip install transformers peft accelerate bitsandbytes gradio torch
   ```

2. Lancez l'interface web :
   ```bash
   python chat_gradio.py
   ```

3. Ouvrez votre navigateur sur `http://localhost:7860` üéâ

## üìä Format du dataset

Votre fichier `combined_dataset.jsonl` doit contenir des dialogues au format OpenAI :

```jsonl
{"messages": [{"role": "user", "content": "Bonjour, comment vas-tu ?"}, {"role": "assistant", "content": "Je vais bien, merci ! Et toi ?"}]}
{"messages": [{"role": "user", "content": "Explique-moi le machine learning"}, {"role": "assistant", "content": "Le machine learning est une branche de l'IA..."}]}
```

### Exemple de dataset

Voir [`examples/example_dataset.jsonl`](examples/example_dataset.jsonl) pour un exemple de 10 dialogues vari√©s.

> **Recommandation** : 100-200+ dialogues de qualit√© pour de bons r√©sultats

## üß† Pourquoi Vigostral-7B-Chat ?

| Aspect | Vigostral-7B-Chat | GPT-2 Fran√ßais |
|--------|-------------------|----------------|
| **Taille** | 7 milliards de params | 1.7 milliards |
| **Entra√Ænement** | 213k dialogues fran√ßais | Texte g√©n√©rique |
| **Capacit√© conversationnelle** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | ‚≠ê‚≠ê Limit√© |
| **Architecture** | Mistral-7B (SOTA) | GPT-2 (2019) |
| **Multi-turn chat** | ‚úÖ Oui | ‚ùå Non natif |

**Vigostral est sp√©cifiquement entra√Æn√© pour tenir des conversations naturelles en fran√ßais.**

## ‚öôÔ∏è Technique : LoRA (Low-Rank Adaptation)

**Probl√®me** : Fine-tuner 7 milliards de param√®tres n√©cessite ~50GB de VRAM (impossible sur T4).

**Solution** : **LoRA** ne fine-tune que ~1% des param√®tres (~70M) en ajoutant de petites matrices entra√Ænables.

**Avantages** :
- ‚úÖ Tient dans 16GB (GPU T4 gratuit)
- ‚úÖ Entra√Ænement 3-5√ó plus rapide
- ‚úÖ Qualit√© comparable au fine-tuning complet
- ‚úÖ Fichiers d'adaptateurs l√©gers (~100-200 MB)

## üìÅ Structure du projet

```
nanochat-french-tutorial/
‚îú‚îÄ‚îÄ vigostral_finetune_colab.ipynb  # Notebook Colab (fine-tuning)
‚îú‚îÄ‚îÄ chat_gradio.py                  # Interface web locale
‚îú‚îÄ‚îÄ INTEGRATION_NANOCHAT.md         # Guide d'int√©gration avec nanochat
‚îú‚îÄ‚îÄ README.md                        # Ce fichier
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ example_dataset.jsonl       # Exemple de dataset
‚îî‚îÄ‚îÄ .gitignore
```

## üìä Performances attendues

### Sur GPU T4 (Google Colab gratuit)

| M√©trique | Valeur |
|----------|--------|
| **Temps d'installation** | 2-3 minutes |
| **Temps de fine-tuning** (123 dialogues, 3 epochs) | 20-30 minutes |
| **Taille mod√®le base** | ~4 GB (quantization 4-bit) |
| **Taille adaptateurs LoRA** | ~100-200 MB |
| **Co√ªt** | 0‚Ç¨ (GPU gratuit) |

### Comparaison CPU vs GPU

- **CPU (MacBook Pro)** : ‚ùå Impraticable (plusieurs heures bloqu√©)
- **GPU T4 (Colab)** : ‚úÖ 20-30 minutes
- **Speedup** : **~100-200√ó plus rapide** ! üöÄ

## üõ†Ô∏è Technologies utilis√©es

- [Vigostral-7B-Chat](https://huggingface.co/vigostral/vigostral-7b-chat) - Mod√®le conversationnel fran√ßais
- [PEFT (LoRA)](https://huggingface.co/docs/peft) - Fine-tuning efficace
- [Transformers](https://huggingface.co/docs/transformers) - Biblioth√®que HuggingFace
- [Gradio](https://gradio.app/) - Interface web moderne
- [Google Colab](https://colab.research.google.com/) - GPU gratuit
- [PyTorch](https://pytorch.org/) - Framework deep learning

## üéì Ce que vous apprendrez

1. **Fine-tuning avec LoRA** : Adapter un grand mod√®le avec peu de ressources
2. **Mod√®les conversationnels** : Diff√©rence entre GPT-2 et mod√®les de chat
3. **Quantization 4-bit** : R√©duire la m√©moire sans perdre de qualit√©
4. **Google Colab** : Utiliser des GPUs gratuits efficacement
5. **Gradio** : Cr√©er des interfaces web pour vos mod√®les
6. **HuggingFace Transformers** : Charger et utiliser des mod√®les open source

## üÜò Probl√®mes courants

### Le GPU n'est pas activ√© sur Colab
**Solution** : Runtime ‚Üí Change runtime type ‚Üí S√©lectionnez **T4 GPU**

### "Out of Memory" pendant le training
**Solution** :
- R√©duisez `per_device_train_batch_size` de 1 √† ... attendez, c'est d√©j√† √† 1 !
- Augmentez `gradient_accumulation_steps` de 4 √† 8
- R√©duisez `max_seq_length` de 512 √† 256

### Le mod√®le r√©pond toujours la m√™me chose
**Solutions** :
- Augmentez la **temperature** (de 0.7 √† 0.9)
- Ajoutez plus de dialogues vari√©s dans votre dataset
- Augmentez le nombre d'epochs (de 3 √† 5)

### Le t√©l√©chargement du mod√®le √©choue
**Solution** :
- Cr√©ez un token HuggingFace (gratuit) sur https://huggingface.co/settings/tokens
- Acceptez les conditions d'utilisation de Vigostral sur HuggingFace

### L'interface Gradio ne se lance pas localement
**Solution** :
- V√©rifiez que les adaptateurs LoRA sont dans `./vigostral-finetuned-final/`
- Si vous n'avez pas de GPU local, le mod√®le tournera sur CPU (lent mais fonctionnel)

## üîó Int√©gration avec Nanochat

Vous voulez utiliser votre mod√®le avec l'interface compl√®te de **Nanochat** (Andrej Karpathy) ?

üëâ Consultez le guide d√©taill√© : [`INTEGRATION_NANOCHAT.md`](INTEGRATION_NANOCHAT.md)

**TL;DR** : Nanochat utilise une architecture GPT custom incompatible avec Vigostral (Mistral). Deux options :
- **Option A** : Cr√©er un wrapper pour charger des mod√®les HuggingFace dans nanochat
- **Option C** : Utiliser l'interface Gradio standalone fournie (recommand√© pour commencer)

## üìö Ressources compl√©mentaires

- [Documentation Vigostral](https://huggingface.co/vigostral/vigostral-7b-chat)
- [Tutoriel LoRA officiel](https://huggingface.co/blog/lora)
- [Documentation PEFT](https://huggingface.co/docs/peft)
- [Documentation Gradio](https://gradio.app/docs/)
- [Nanochat (Andrej Karpathy)](https://github.com/karpathy/nanochat)

## ü§ù Contribuer

Les contributions sont les bienvenues ! Pour contribuer :

1. Forkez ce repo
2. Cr√©ez une branche (`git checkout -b feature/am√©lioration`)
3. Committez vos changements (`git commit -m 'Ajout d'une fonctionnalit√©'`)
4. Pushez (`git push origin feature/am√©lioration`)
5. Ouvrez une Pull Request

## ‚ùì FAQ

### Q : Combien de dialogues faut-il pour de bons r√©sultats ?
**R** : Minimum 50, id√©al 100-200+. La qualit√© est plus importante que la quantit√©.

### Q : Puis-je utiliser ce mod√®le commercialement ?
**R** : Vigostral-7B-Chat est sous licence Apache 2.0 (usage commercial autoris√©). V√©rifiez toujours les licences.

### Q : Le mod√®le fonctionne-t-il hors ligne ?
**R** : Oui ! Une fois t√©l√©charg√©, tout fonctionne localement sans internet.

### Q : Quelle est la diff√©rence entre ce projet et nanochat ?
**R** :
- **Nanochat** : Framework complet pour entra√Æner un LLM from scratch (co√ªte $100, 4h sur 8√óH100)
- **Ce projet** : Fine-tune un mod√®le fran√ßais existant sur vos dialogues (gratuit, 30 min sur T4)

### Q : Puis-je fine-tuner sur d'autres langues ?
**R** : Oui ! Remplacez Vigostral par un autre mod√®le (ex: [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) pour l'anglais).

## ‚ú® Exemples de r√©sultats

Apr√®s fine-tuning sur 123 dialogues techniques :

**Prompt** : *"Explique-moi ton projet principal"*

**Avant fine-tuning (mod√®le de base)** :
```
Mon projet principal consiste √† d√©velopper des solutions d'IA...
[r√©ponse g√©n√©rique]
```

**Apr√®s fine-tuning (votre style)** :
```
Mon projet principal c'est Geofen, une plateforme de jeu en ligne
o√π les joueurs peuvent cr√©er leurs propres m√©caniques de jeu avec
un syst√®me de r√®gles modulaires. J'utilise Python et Flask pour
le backend, et j'exp√©rimente avec des IA pour g√©n√©rer du contenu...
[r√©ponse personnalis√©e avec votre vocabulaire et vos projets]
```

---

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

Le mod√®le [Vigostral-7B-Chat](https://huggingface.co/vigostral/vigostral-7b-chat) est sous licence Apache 2.0.

## üôè Remerciements

- [Vigogne AI](https://github.com/bofenghuang/vigogne) pour Vigostral-7B-Chat
- [Mistral AI](https://mistral.ai/) pour l'architecture Mistral-7B
- [HuggingFace](https://huggingface.co/) pour les outils et l'infrastructure
- [Andrej Karpathy](https://github.com/karpathy) pour l'inspiration (nanochat)
- La communaut√© IA fran√ßaise üá´üá∑

---

**Made with ‚ù§Ô∏è for the French AI community**

Pour toute question, ouvrez une [issue](https://github.com/cladjidane/nanochat-french-tutorial/issues) !
