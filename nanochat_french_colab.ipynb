{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üá´üá∑ Fine-tuning GPT-2 Fran√ßais avec Nanochat\n",
    "\n",
    "Ce notebook vous permet de fine-tuner le mod√®le GPT-2 fran√ßais (`asi/gpt-fr-cased-base`) sur vos dialogues pr√©par√©s via **Pipeline Manager**.\n",
    "\n",
    "## üìã Pr√©requis\n",
    "\n",
    "Avant de lancer ce notebook, assurez-vous d'avoir :\n",
    "1. ‚úÖ Pr√©par√© votre dataset via Pipeline Manager (local)\n",
    "2. ‚úÖ Export√© le fichier `combined_dataset.jsonl`\n",
    "3. ‚úÖ Le fichier contient des dialogues au format JSON avec `messages`\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT : Activez le GPU !**\n",
    "1. Menu `Runtime` ‚Üí `Change runtime type`\n",
    "2. S√©lectionnez `T4 GPU`\n",
    "3. Cliquez `Save`\n",
    "\n",
    "**Temps estim√© :** ~10 minutes pour 300 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1Ô∏è‚É£ Installation des d√©pendances\n",
    "\n",
    "Installation de l'environnement complet (2-3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# V√©rifier le GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Installer Rust (requis pour le tokenizer)\n",
    "print(\"üì¶ Installation de Rust...\")\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Cloner le repo nanochat\n",
    "print(\"\\nüì• Clonage du repo nanochat...\")\n",
    "!git clone https://github.com/keller-jordan/nanochat.git\n",
    "%cd nanochat\n",
    "\n",
    "# Checkout de la branche French\n",
    "!git checkout feat/french-experiment\n",
    "\n",
    "# Installer les d√©pendances Python\n",
    "print(\"\\nüì¶ Installation des d√©pendances Python...\")\n",
    "!pip install -q torch transformers tiktoken maturin\n",
    "\n",
    "# Compiler le tokenizer Rust\n",
    "print(\"\\nüî® Compilation du tokenizer Rust...\")\n",
    "!maturin develop --release --manifest-path rustbpe/Cargo.toml\n",
    "\n",
    "print(\"\\n‚úÖ Installation termin√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-header"
   },
   "source": [
    "## 2Ô∏è‚É£ Upload de votre dataset\n",
    "\n",
    "Uploadez votre fichier `combined_dataset.jsonl` g√©n√©r√© par Pipeline Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-data"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Cr√©er la structure de dossiers\n",
    "data_dir = Path(\"data/Stage.FINAL\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Upload du fichier\n",
    "print(\"üìÅ S√©lectionnez votre fichier combined_dataset.jsonl...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# D√©placer le fichier upload√©\n",
    "for filename in uploaded.keys():\n",
    "    target_path = data_dir / \"combined_dataset.jsonl\"\n",
    "    shutil.move(filename, target_path)\n",
    "    print(f\"‚úÖ Fichier {filename} copi√© dans {target_path}\")\n",
    "\n",
    "# V√©rifier le contenu\n",
    "import json\n",
    "dialogues = []\n",
    "with open(target_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            dialogues.append(json.loads(line))\n",
    "\n",
    "print(f\"\\nüìä Statistiques du dataset :\")\n",
    "print(f\"  - Total dialogues: {len(dialogues)}\")\n",
    "print(f\"  - Split train (90%): ~{int(len(dialogues) * 0.9)}\")\n",
    "print(f\"  - Split val (10%): ~{int(len(dialogues) * 0.1)}\")\n",
    "\n",
    "# Afficher un exemple\n",
    "if dialogues:\n",
    "    print(f\"\\nüìù Exemple de dialogue :\")\n",
    "    example = dialogues[0]\n",
    "    for msg in example.get('messages', [])[:4]:  # Premiers 4 messages\n",
    "        role = msg['role'].upper()\n",
    "        content = msg['content'][:100]  # Premiers 100 chars\n",
    "        print(f\"  {role}: {content}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-model-header"
   },
   "source": [
    "## 3Ô∏è‚É£ T√©l√©chargement du mod√®le GPT-2 Fran√ßais\n",
    "\n",
    "T√©l√©chargement du mod√®le pr√©-entra√Æn√© depuis HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-model"
   },
   "outputs": [],
   "source": [
    "print(\"üì• T√©l√©chargement du mod√®le GPT-2 Fran√ßais (asi/gpt-fr-cased-base)...\")\n",
    "print(\"‚è±Ô∏è  Cela peut prendre 2-3 minutes...\\n\")\n",
    "\n",
    "!python -m scripts.download_gpt2_french\n",
    "\n",
    "print(\"\\n‚úÖ Mod√®le t√©l√©charg√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-header"
   },
   "source": [
    "## 4Ô∏è‚É£ Configuration du training\n",
    "\n",
    "Param√®tres par d√©faut (modifiables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# Param√®tres de training\n",
    "NUM_EPOCHS = 3                # Nombre d'√©poques (3 = bon compromis)\n",
    "TARGET_EXAMPLES_PER_STEP = 8  # Nombre d'exemples par step (8 pour GPU)\n",
    "DEVICE_BATCH_SIZE = 32        # Batch size (32 devrait passer sur T4 16GB)\n",
    "EMBEDDING_LR = 0.2            # Learning rate pour embeddings\n",
    "MATRIX_LR = 0.02              # Learning rate pour matrices\n",
    "UNEMBEDDING_LR = 0.004        # Learning rate pour lm_head\n",
    "EVAL_EVERY = 10               # √âvaluation tous les N steps\n",
    "\n",
    "# Calculer le nombre d'it√©rations approximatif\n",
    "num_dialogues = len(dialogues)\n",
    "train_dialogues = int(num_dialogues * 0.9)\n",
    "approx_iterations = (train_dialogues // TARGET_EXAMPLES_PER_STEP) * NUM_EPOCHS\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration :\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Examples per step: {TARGET_EXAMPLES_PER_STEP}\")\n",
    "print(f\"  - Device batch size: {DEVICE_BATCH_SIZE}\")\n",
    "print(f\"  - Training dialogues: ~{train_dialogues}\")\n",
    "print(f\"  - Iterations approximatives: ~{approx_iterations}\")\n",
    "print(f\"  - Temps estim√© (T4): ~{approx_iterations * 15 / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-header"
   },
   "source": [
    "## 5Ô∏è‚É£ Lancement du training\n",
    "\n",
    "Le training va commencer. Vous verrez les logs en temps r√©el.\n",
    "\n",
    "**Ce que vous allez voir :**\n",
    "- Chargement du mod√®le GPT-2 fran√ßais\n",
    "- Chargement du tokenizer\n",
    "- Chargement du dataset\n",
    "- Progress avec validation loss tous les 10 steps\n",
    "- Sauvegarde automatique du checkpoint final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Lancer le training\n",
    "!python -m scripts.sft_gpt2_french \\\n",
    "  --num_epochs={NUM_EPOCHS} \\\n",
    "  --target_examples_per_step={TARGET_EXAMPLES_PER_STEP} \\\n",
    "  --device_batch_size={DEVICE_BATCH_SIZE} \\\n",
    "  --embedding_lr={EMBEDDING_LR} \\\n",
    "  --matrix_lr={MATRIX_LR} \\\n",
    "  --unembedding_lr={UNEMBEDDING_LR} \\\n",
    "  --eval_every={EVAL_EVERY} \\\n",
    "  --dtype=bfloat16\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training termin√© en {elapsed/60:.1f} minutes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-header"
   },
   "source": [
    "## 6Ô∏è‚É£ Test du mod√®le fine-tun√©\n",
    "\n",
    "Testons le mod√®le avec quelques prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-model"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Trouver le dernier checkpoint\n",
    "print(\"üì• Chargement du mod√®le fine-tun√©...\")\n",
    "\n",
    "checkpoint_dir = Path.home() / \".cache/nanochat/sft_french_checkpoints\"\n",
    "model_checkpoints = sorted(checkpoint_dir.glob(\"*/model_*.pt\"))\n",
    "\n",
    "if model_checkpoints:\n",
    "    latest_checkpoint = model_checkpoints[-1]\n",
    "    print(f\"‚úÖ Checkpoint trouv√© : {latest_checkpoint}\")\n",
    "    \n",
    "    # Charger le mod√®le de base\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
    "    \n",
    "    # Charger les poids fine-tun√©s\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location='cuda')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
    "    \n",
    "    # Fonction de g√©n√©ration\n",
    "    def generate(prompt, max_length=150, temperature=0.8, top_p=0.9):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Tests\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ Test du mod√®le fine-tun√©\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Bonjour, comment allez-vous ?\",\n",
    "        \"Expliquez-moi le machine learning\",\n",
    "        \"Quelle est la capitale de la France ?\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nüìù Prompt: \\\"{prompt}\\\"\")\n",
    "        result = generate(prompt)\n",
    "        print(f\"ü§ñ G√©n√©ration: {result}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå Aucun checkpoint trouv√©. Le training a-t-il r√©ussi ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-header"
   },
   "source": [
    "## 7Ô∏è‚É£ Mode interactif\n",
    "\n",
    "Testez le mod√®le avec vos propres prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive"
   },
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "print(\"üí¨ Mode interactif activ√© !\")\n",
    "print(\"Entrez vos prompts ci-dessous (laissez vide pour arr√™ter)\\n\")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"\\nüßë Votre prompt : \")\n",
    "    if not prompt.strip():\n",
    "        break\n",
    "    \n",
    "    result = generate(prompt, max_length=200)\n",
    "    print(f\"\\nü§ñ {result}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-header"
   },
   "source": [
    "## 8Ô∏è‚É£ T√©l√©charger le checkpoint\n",
    "\n",
    "T√©l√©chargez le mod√®le fine-tun√© sur votre machine locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Trouver tous les checkpoints\n",
    "checkpoint_dir = Path.home() / \".cache/nanochat/sft_french_checkpoints\"\n",
    "model_files = sorted(checkpoint_dir.glob(\"*/model_*.pt\"))\n",
    "meta_files = sorted(checkpoint_dir.glob(\"*/meta_*.json\"))\n",
    "\n",
    "print(f\"üì¶ Fichiers trouv√©s :\")\n",
    "print(f\"  - {len(model_files)} model checkpoints\")\n",
    "print(f\"  - {len(meta_files)} metadata files\")\n",
    "\n",
    "if model_files:\n",
    "    # T√©l√©charger le dernier checkpoint\n",
    "    latest_model = model_files[-1]\n",
    "    latest_meta = meta_files[-1] if meta_files else None\n",
    "    \n",
    "    print(f\"\\n‚¨áÔ∏è T√©l√©chargement du dernier checkpoint...\")\n",
    "    files.download(str(latest_model))\n",
    "    \n",
    "    if latest_meta:\n",
    "        files.download(str(latest_meta))\n",
    "    \n",
    "    print(\"\\n‚úÖ Checkpoint t√©l√©charg√© !\")\n",
    "    print(f\"üìÅ Fichier: {latest_model.name}\")\n",
    "    \n",
    "    # Afficher la taille\n",
    "    size_mb = latest_model.stat().st_size / (1024*1024)\n",
    "    print(f\"üìä Taille: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun checkpoint trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stats-header"
   },
   "source": [
    "## üìä Statistiques du training\n",
    "\n",
    "R√©sum√© des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stats"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Charger le dernier meta.json\n",
    "checkpoint_dir = Path.home() / \".cache/nanochat/sft_french_checkpoints\"\n",
    "meta_files = sorted(checkpoint_dir.glob(\"*/meta_*.json\"))\n",
    "\n",
    "if meta_files:\n",
    "    with open(meta_files[-1], 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä R√âSUM√â DU TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n‚úÖ Step final: {meta.get('step', 'N/A')}\")\n",
    "    print(f\"‚úÖ Loss final: {meta.get('val_loss', 'N/A'):.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Mod√®le: GPT-2 French (asi/gpt-fr-cased-base)\")\n",
    "    print(f\"üéØ Dataset: {len(dialogues)} dialogues\")\n",
    "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üéØ Epochs: {NUM_EPOCHS}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå Aucun fichier meta.json trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Prochaines √©tapes\n",
    "\n",
    "Votre mod√®le est maintenant fine-tun√© ! Vous pouvez :\n",
    "\n",
    "1. **Continuer le training** : Augmentez `NUM_EPOCHS` ou uploadez plus de dialogues\n",
    "2. **Tester diff√©rents prompts** : Utilisez la cellule interactive\n",
    "3. **T√©l√©charger le checkpoint** : Pour l'utiliser localement\n",
    "4. **Ajuster les hyperparam√®tres** : Modifiez learning rates, batch size, etc.\n",
    "\n",
    "### Utiliser le checkpoint localement\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Charger le mod√®le de base\n",
    "model = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
    "\n",
    "# Charger les poids fine-tun√©s\n",
    "checkpoint = torch.load('model_000300.pt')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "\n",
    "# G√©n√©rer du texte\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
    "# ... g√©n√©ration ...\n",
    "```\n",
    "\n",
    "### Workflow complet recommand√©\n",
    "\n",
    "```\n",
    "LOCAL (Mac) :\n",
    "‚îú‚îÄ Pipeline Manager (http://localhost:8800)\n",
    "‚îÇ  ‚îú‚îÄ Audio ‚Üí Transcription ‚Üí √âdition\n",
    "‚îÇ  ‚îú‚îÄ G√©n√©ration dialogues (DeepSeek API)\n",
    "‚îÇ  ‚îú‚îÄ Scoring automatique\n",
    "‚îÇ  ‚îî‚îÄ Export combined_dataset.jsonl\n",
    "‚îÇ\n",
    "COLAB (GPU gratuit) :\n",
    "‚îú‚îÄ Upload combined_dataset.jsonl\n",
    "‚îú‚îÄ Fine-tuning (~10 min sur T4)\n",
    "‚îî‚îÄ Download checkpoint\n",
    "```\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- üìö Pipeline Manager : `QUICKSTART_PIPELINE.md`\n",
    "- üèóÔ∏è Architecture : `ARCHITECTURE_GPT2_VS_NANOCHAT.md`\n",
    "- üîß Troubleshooting : `TROUBLESHOOTING_FR.md`\n",
    "- ‚òÅÔ∏è Guide cloud GPU : `GUIDE_CLOUD_GPU.md`\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations ! Vous avez maintenant un mod√®le GPT-2 fran√ßais personnalis√© !**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
