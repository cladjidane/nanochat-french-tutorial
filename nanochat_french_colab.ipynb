{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üá´üá∑ Fine-tuning GPT-2 Fran√ßais avec Nanochat\n",
        "\n",
        "Ce notebook vous permet de fine-tuner le mod√®le GPT-2 fran√ßais (`asi/gpt-fr-cased-base`) sur vos dialogues pr√©par√©s via **Pipeline Manager**.\n",
        "\n",
        "## üìã Pr√©requis\n",
        "\n",
        "Avant de lancer ce notebook, assurez-vous d'avoir :\n",
        "1. ‚úÖ Pr√©par√© votre dataset via Pipeline Manager (local)\n",
        "2. ‚úÖ Export√© le fichier `combined_dataset.jsonl`\n",
        "3. ‚úÖ Le fichier contient des dialogues au format JSON avec `messages`\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT : Activez le GPU !**\n",
        "1. Menu `Runtime` ‚Üí `Change runtime type`\n",
        "2. S√©lectionnez `T4 GPU`\n",
        "3. Cliquez `Save`\n",
        "\n",
        "**Temps estim√© :** ~10 minutes pour 300 iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## 1Ô∏è‚É£ Installation des d√©pendances\n",
        "\n",
        "Installation de l'environnement complet (2-3 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install-deps",
        "outputId": "b9832d25-e79a-42f8-8702-53ea8ee6c175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 17 08:33:55 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "üì¶ Installation de Rust...\n",
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2025-09-18, rust version 1.90.0 (1159e78c4 2025-09-14)\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
            " 20.5 MiB /  20.5 MiB (100 %)   4.4 MiB/s in  3s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
            " 27.8 MiB /  27.8 MiB (100 %)   8.2 MiB/s in  4s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
            " 78.7 MiB /  78.7 MiB (100 %)  10.8 MiB/s in  7s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.90.0 (1159e78c4 2025-09-14)\n",
            "\n",
            "\u001b[0m\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[0m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, you need to source\n",
            "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
            "\n",
            "This is usually done by running one of the following (note the leading DOT):\n",
            ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
            "source \"$HOME/.cargo/env.fish\"  # For fish\n",
            "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
            "\n",
            "üì• Clonage du repo nanochat...\n",
            "Cloning into 'nanochat'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "[Errno 2] No such file or directory: 'nanochat'\n",
            "/content\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\n",
            "üì¶ Installation des d√©pendances Python...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "üî® Compilation du tokenizer Rust...\n",
            "üí• maturin failed\n",
            "  Caused by: Couldn't find a virtualenv or conda environment, but you need one to use this command. For maturin to find your virtualenv you need to either set VIRTUAL_ENV (through activate), set CONDA_PREFIX (through conda activate) or have a virtualenv called .venv in the current or any parent folder. See https://virtualenv.pypa.io/en/latest/index.html on how to use virtualenv or use `maturin build` and `pip install <path/to/wheel>` instead.\n",
            "\n",
            "‚úÖ Installation termin√©e !\n"
          ]
        }
      ],
      "source": [
        "# V√©rifier le GPU\n",
        "!nvidia-smi\n",
        "\n",
        "# Installer Rust (requis pour le tokenizer)\n",
        "print(\"üì¶ Installation de Rust...\")\n",
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "import os\n",
        "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
        "\n",
        "# Cloner le repo nanochat\n",
        "print(\"\\nüì• Clonage du repo nanochat...\")\n",
        "!git clone https://github.com/karpathy/nanochat\n",
        "%cd nanochat\n",
        "\n",
        "# Checkout de la branche French\n",
        "!git checkout feat/french-experiment\n",
        "\n",
        "# Installer les d√©pendances Python\n",
        "print(\"\\nüì¶ Installation des d√©pendances Python...\")\n",
        "!pip install -q torch transformers tiktoken maturin\n",
        "\n",
        "# Compiler le tokenizer Rust\n",
        "print(\"\\nüî® Compilation du tokenizer Rust...\")\n",
        "!maturin develop --release --manifest-path rustbpe/Cargo.toml\n",
        "\n",
        "print(\"\\n‚úÖ Installation termin√©e !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload-header"
      },
      "source": [
        "## 2Ô∏è‚É£ Upload de votre dataset\n",
        "\n",
        "Uploadez votre fichier `combined_dataset.jsonl` g√©n√©r√© par Pipeline Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload-data"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Cr√©er la structure de dossiers\n",
        "data_dir = Path(\"data/Stage.FINAL\")\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Upload du fichier\n",
        "print(\"üìÅ S√©lectionnez votre fichier combined_dataset.jsonl...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# D√©placer le fichier upload√©\n",
        "for filename in uploaded.keys():\n",
        "    target_path = data_dir / \"combined_dataset.jsonl\"\n",
        "    shutil.move(filename, target_path)\n",
        "    print(f\"‚úÖ Fichier {filename} copi√© dans {target_path}\")\n",
        "\n",
        "# V√©rifier le contenu\n",
        "import json\n",
        "dialogues = []\n",
        "with open(target_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            dialogues.append(json.loads(line))\n",
        "\n",
        "print(f\"\\nüìä Statistiques du dataset :\")\n",
        "print(f\"  - Total dialogues: {len(dialogues)}\")\n",
        "print(f\"  - Split train (90%): ~{int(len(dialogues) * 0.9)}\")\n",
        "print(f\"  - Split val (10%): ~{int(len(dialogues) * 0.1)}\")\n",
        "\n",
        "# Afficher un exemple\n",
        "if dialogues:\n",
        "    print(f\"\\nüìù Exemple de dialogue :\")\n",
        "    example = dialogues[0]\n",
        "    for msg in example.get('messages', [])[:4]:  # Premiers 4 messages\n",
        "        role = msg['role'].upper()\n",
        "        content = msg['content'][:100]  # Premiers 100 chars\n",
        "        print(f\"  {role}: {content}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-model-header"
      },
      "source": [
        "## 3Ô∏è‚É£ T√©l√©chargement du mod√®le GPT-2 Fran√ßais\n",
        "\n",
        "T√©l√©chargement du mod√®le pr√©-entra√Æn√© depuis HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-model"
      },
      "outputs": [],
      "source": [
        "print(\"üì• T√©l√©chargement du mod√®le GPT-2 Fran√ßais (asi/gpt-fr-cased-base)...\")\n",
        "print(\"‚è±Ô∏è  Cela peut prendre 2-3 minutes...\\n\")\n",
        "\n",
        "!python -m scripts.download_gpt2_french\n",
        "\n",
        "print(\"\\n‚úÖ Mod√®le t√©l√©charg√© !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "## 4Ô∏è‚É£ Configuration du training\n",
        "\n",
        "Param√®tres par d√©faut (modifiables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# Param√®tres de training\n",
        "NUM_EPOCHS = 3                # Nombre d'√©poques (3 = bon compromis)\n",
        "TARGET_EXAMPLES_PER_STEP = 8  # Nombre d'exemples par step (8 pour GPU)\n",
        "DEVICE_BATCH_SIZE = 32        # Batch size (32 devrait passer sur T4 16GB)\n",
        "EMBEDDING_LR = 0.2            # Learning rate pour embeddings\n",
        "MATRIX_LR = 0.02              # Learning rate pour matrices\n",
        "UNEMBEDDING_LR = 0.004        # Learning rate pour lm_head\n",
        "EVAL_EVERY = 10               # √âvaluation tous les N steps\n",
        "\n",
        "# Calculer le nombre d'it√©rations approximatif\n",
        "num_dialogues = len(dialogues)\n",
        "train_dialogues = int(num_dialogues * 0.9)\n",
        "approx_iterations = (train_dialogues // TARGET_EXAMPLES_PER_STEP) * NUM_EPOCHS\n",
        "\n",
        "print(\"‚öôÔ∏è Configuration :\")\n",
        "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  - Examples per step: {TARGET_EXAMPLES_PER_STEP}\")\n",
        "print(f\"  - Device batch size: {DEVICE_BATCH_SIZE}\")\n",
        "print(f\"  - Training dialogues: ~{train_dialogues}\")\n",
        "print(f\"  - Iterations approximatives: ~{approx_iterations}\")\n",
        "print(f\"  - Temps estim√© (T4): ~{approx_iterations * 15 / 60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-header"
      },
      "source": [
        "## 5Ô∏è‚É£ Lancement du training\n",
        "\n",
        "Le training va commencer. Vous verrez les logs en temps r√©el.\n",
        "\n",
        "**Ce que vous allez voir :**\n",
        "- Chargement du mod√®le GPT-2 fran√ßais\n",
        "- Chargement du tokenizer\n",
        "- Chargement du dataset\n",
        "- Progress avec validation loss tous les 10 steps\n",
        "- Sauvegarde automatique du checkpoint final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Lancer le training\n",
        "!python -m scripts.sft_gpt2_french \\\n",
        "  --num_epochs={NUM_EPOCHS} \\\n",
        "  --target_examples_per_step={TARGET_EXAMPLES_PER_STEP} \\\n",
        "  --device_batch_size={DEVICE_BATCH_SIZE} \\\n",
        "  --embedding_lr={EMBEDDING_LR} \\\n",
        "  --matrix_lr={MATRIX_LR} \\\n",
        "  --unembedding_lr={UNEMBEDDING_LR} \\\n",
        "  --eval_every={EVAL_EVERY} \\\n",
        "  --dtype=bfloat16\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Training termin√© en {elapsed/60:.1f} minutes !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-header"
      },
      "source": [
        "## 6Ô∏è‚É£ Test du mod√®le fine-tun√©\n",
        "\n",
        "Testons le mod√®le avec quelques prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Trouver le dernier checkpoint\n",
        "print(\"üì• Chargement du mod√®le fine-tun√©...\")\n",
        "\n",
        "checkpoint_dir = Path.home() / \".cache/nanochat/sft_french_checkpoints\"\n",
        "model_checkpoints = sorted(checkpoint_dir.glob(\"*/model_*.pt\"))\n",
        "\n",
        "if model_checkpoints:\n",
        "    latest_checkpoint = model_checkpoints[-1]\n",
        "    print(f\"‚úÖ Checkpoint trouv√© : {latest_checkpoint}\")\n",
        "\n",
        "    # Charger le mod√®le de base\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
        "\n",
        "    # Charger les poids fine-tun√©s\n",
        "    checkpoint = torch.load(latest_checkpoint, map_location='cuda')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
        "\n",
        "    # Fonction de g√©n√©ration\n",
        "    def generate(prompt, max_length=150, temperature=0.8, top_p=0.9):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=top_p,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Tests\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ü§ñ Test du mod√®le fine-tun√©\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_prompts = [\n",
        "        \"Bonjour, comment allez-vous ?\",\n",
        "        \"Expliquez-moi le machine learning\",\n",
        "        \"Quelle est la capitale de la France ?\"\n",
        "    ]\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        print(f\"\\nüìù Prompt: \\\"{prompt}\\\"\")\n",
        "        result = generate(prompt)\n",
        "        print(f\"ü§ñ G√©n√©ration: {result}\")\n",
        "        print(\"-\" * 60)\n",
        "else:\n",
        "    print(\"‚ùå Aucun checkpoint trouv√©. Le training a-t-il r√©ussi ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive-header"
      },
      "source": [
        "## 7Ô∏è‚É£ Mode interactif\n",
        "\n",
        "Testez le mod√®le avec vos propres prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive"
      },
      "outputs": [],
      "source": [
        "# Mode interactif\n",
        "print(\"üí¨ Mode interactif activ√© !\")\n",
        "print(\"Entrez vos prompts ci-dessous (laissez vide pour arr√™ter)\\n\")\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"\\nüßë Votre prompt : \")\n",
        "    if not prompt.strip():\n",
        "        break\n",
        "\n",
        "    result = generate(prompt, max_length=200)\n",
        "    print(f\"\\nü§ñ {result}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-header"
      },
      "source": [
        "## 8Ô∏è‚É£ T√©l√©charger le checkpoint\n",
        "\n",
        "T√©l√©chargez le mod√®le fine-tun√© sur votre machine locale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Trouver tous les checkpoints\n",
        "checkpoint_dir = Path.home() / \".cache/nanochat/sft_french_checkpoints\"\n",
        "model_files = sorted(checkpoint_dir.glob(\"*/model_*.pt\"))\n",
        "meta_files = sorted(checkpoint_dir.glob(\"*/meta_*.json\"))\n",
        "\n",
        "print(f\"üì¶ Fichiers trouv√©s :\")\n",
        "print(f\"  - {len(model_files)} model checkpoints\")\n",
        "print(f\"  - {len(meta_files)} metadata files\")\n",
        "\n",
        "if model_files:\n",
        "    # T√©l√©charger le dernier checkpoint\n",
        "    latest_model = model_files[-1]\n",
        "    latest_meta = meta_files[-1] if meta_files else None\n",
        "\n",
        "    print(f\"\\n‚¨áÔ∏è T√©l√©chargement du dernier checkpoint...\")\n",
        "    files.download(str(latest_model))\n",
        "\n",
        "    if latest_meta:\n",
        "        files.download(str(latest_meta))\n",
        "\n",
        "    print(\"\\n‚úÖ Checkpoint t√©l√©charg√© !\")\n",
        "    print(f\"üìÅ Fichier: {latest_model.name}\")\n",
        "\n",
        "    # Afficher la taille\n",
        "    size_mb = latest_model.stat().st_size / (1024*1024)\n",
        "    print(f\"üìä Taille: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(\"‚ùå Aucun checkpoint trouv√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stats-header"
      },
      "source": [
        "## üìä Statistiques du training\n",
        "\n",
        "R√©sum√© des performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stats"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Charger le dernier meta.json\n",
        "checkpoint_dir = Path.home() / \".cache/nanochat/sft_french_checkpoints\"\n",
        "meta_files = sorted(checkpoint_dir.glob(\"*/meta_*.json\"))\n",
        "\n",
        "if meta_files:\n",
        "    with open(meta_files[-1], 'r') as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üìä R√âSUM√â DU TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n‚úÖ Step final: {meta.get('step', 'N/A')}\")\n",
        "    print(f\"‚úÖ Loss final: {meta.get('val_loss', 'N/A'):.4f}\")\n",
        "\n",
        "    print(f\"\\nüéØ Mod√®le: GPT-2 French (asi/gpt-fr-cased-base)\")\n",
        "    print(f\"üéØ Dataset: {len(dialogues)} dialogues\")\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üéØ Epochs: {NUM_EPOCHS}\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"‚ùå Aucun fichier meta.json trouv√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next-steps"
      },
      "source": [
        "## üéØ Prochaines √©tapes\n",
        "\n",
        "Votre mod√®le est maintenant fine-tun√© ! Vous pouvez :\n",
        "\n",
        "1. **Continuer le training** : Augmentez `NUM_EPOCHS` ou uploadez plus de dialogues\n",
        "2. **Tester diff√©rents prompts** : Utilisez la cellule interactive\n",
        "3. **T√©l√©charger le checkpoint** : Pour l'utiliser localement\n",
        "4. **Ajuster les hyperparam√®tres** : Modifiez learning rates, batch size, etc.\n",
        "\n",
        "### Utiliser le checkpoint localement\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Charger le mod√®le de base\n",
        "model = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
        "\n",
        "# Charger les poids fine-tun√©s\n",
        "checkpoint = torch.load('model_000300.pt')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()\n",
        "\n",
        "# G√©n√©rer du texte\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
        "# ... g√©n√©ration ...\n",
        "```\n",
        "\n",
        "### Workflow complet recommand√©\n",
        "\n",
        "```\n",
        "LOCAL (Mac) :\n",
        "‚îú‚îÄ Pipeline Manager (http://localhost:8800)\n",
        "‚îÇ  ‚îú‚îÄ Audio ‚Üí Transcription ‚Üí √âdition\n",
        "‚îÇ  ‚îú‚îÄ G√©n√©ration dialogues (DeepSeek API)\n",
        "‚îÇ  ‚îú‚îÄ Scoring automatique\n",
        "‚îÇ  ‚îî‚îÄ Export combined_dataset.jsonl\n",
        "‚îÇ\n",
        "COLAB (GPU gratuit) :\n",
        "‚îú‚îÄ Upload combined_dataset.jsonl\n",
        "‚îú‚îÄ Fine-tuning (~10 min sur T4)\n",
        "‚îî‚îÄ Download checkpoint\n",
        "```\n",
        "\n",
        "### Ressources\n",
        "\n",
        "- üìö Pipeline Manager : `QUICKSTART_PIPELINE.md`\n",
        "- üèóÔ∏è Architecture : `ARCHITECTURE_GPT2_VS_NANOCHAT.md`\n",
        "- üîß Troubleshooting : `TROUBLESHOOTING_FR.md`\n",
        "- ‚òÅÔ∏è Guide cloud GPU : `GUIDE_CLOUD_GPU.md`\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ F√©licitations ! Vous avez maintenant un mod√®le GPT-2 fran√ßais personnalis√© !**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}