{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üá´üá∑ Fine-tuning Vigostral-7B-Chat sur votre style personnel\n",
    "\n",
    "Ce notebook vous guide pour fine-tuner le mod√®le conversationnel fran√ßais **Vigostral-7B-Chat** sur vos dialogues personnels, en utilisant **LoRA** (Low-Rank Adaptation) pour s'adapter aux contraintes m√©moire du GPU T4 gratuit de Google Colab.\n",
    "\n",
    "## üìã Pr√©requis\n",
    "\n",
    "1. **GPU T4 activ√©** : Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "2. **Dataset de dialogues** au format JSONL\n",
    "3. **Compte HuggingFace** (gratuit) pour t√©l√©charger Vigostral\n",
    "\n",
    "## üéØ Objectif\n",
    "\n",
    "Adapter Vigostral-7B-Chat (mod√®le conversationnel fran√ßais pr√©-entra√Æn√© sur 213k dialogues) √† votre style d'√©criture personnel en fine-tunant sur vos propres conversations.\n",
    "\n",
    "## ‚öôÔ∏è Technique : LoRA (Low-Rank Adaptation)\n",
    "\n",
    "- **Probl√®me** : Fine-tuner 7B param√®tres n√©cessite ~50GB de VRAM (impossible sur T4)\n",
    "- **Solution** : LoRA ne fine-tune que ~1% des param√®tres (~70M)\n",
    "- **Avantage** : Tient dans 16GB, training rapide, qualit√© pr√©serv√©e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Installation des d√©pendances\n",
    "\n",
    "On installe les biblioth√®ques n√©cessaires :\n",
    "- `transformers` : Charger Vigostral depuis HuggingFace\n",
    "- `peft` : Impl√©menter LoRA\n",
    "- `accelerate` : Optimiser l'entra√Ænement GPU\n",
    "- `bitsandbytes` : Quantization 4-bit pour r√©duire la m√©moire\n",
    "- `trl` : Outils de fine-tuning pour LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate bitsandbytes trl datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîë Authentification HuggingFace\n",
    "\n",
    "Vigostral-7B-Chat n√©cessite un token HuggingFace pour t√©l√©charger le mod√®le.\n",
    "\n",
    "1. Allez sur https://huggingface.co/settings/tokens\n",
    "2. Cr√©ez un token (Read access suffit)\n",
    "3. Collez-le ci-dessous quand demand√©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì§ Upload de votre dataset\n",
    "\n",
    "Uploadez votre fichier `combined_dataset.jsonl` (format : une ligne par dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Uploadez votre fichier combined_dataset.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# V√©rifier que le fichier est bien upload√©\n",
    "if 'combined_dataset.jsonl' not in uploaded:\n",
    "    raise FileNotFoundError(\"Fichier combined_dataset.jsonl non trouv√©. R√©essayez l'upload.\")\n",
    "\n",
    "print(\"‚úÖ Dataset upload√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Chargement et validation du dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger le dataset\n",
    "dialogues = []\n",
    "with open('combined_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        dialogues.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úÖ {len(dialogues)} dialogues charg√©s\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\nüìã Exemple de dialogue :\")\n",
    "print(json.dumps(dialogues[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Conversion au format Vigostral\n",
    "\n",
    "Vigostral utilise un format de prompt sp√©cifique. On doit convertir nos dialogues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue_for_vigostral(dialogue):\n",
    "    \"\"\"\n",
    "    Convertit un dialogue du format OpenAI au format Vigostral.\n",
    "    \n",
    "    Vigostral utilise le format Llama-2 :\n",
    "    <s>[INST] question [/INST] r√©ponse </s>\n",
    "    \"\"\"\n",
    "    messages = dialogue['messages']\n",
    "    formatted_text = \"<s>\"\n",
    "    \n",
    "    for i in range(0, len(messages), 2):\n",
    "        if i < len(messages) and messages[i]['role'] == 'user':\n",
    "            user_msg = messages[i]['content']\n",
    "            formatted_text += f\"[INST] {user_msg} [/INST]\"\n",
    "        \n",
    "        if i + 1 < len(messages) and messages[i + 1]['role'] == 'assistant':\n",
    "            assistant_msg = messages[i + 1]['content']\n",
    "            formatted_text += f\" {assistant_msg} </s>\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Convertir tous les dialogues\n",
    "formatted_dialogues = [format_dialogue_for_vigostral(d) for d in dialogues]\n",
    "\n",
    "print(f\"‚úÖ {len(formatted_dialogues)} dialogues format√©s\")\n",
    "print(\"\\nüìã Exemple de dialogue format√© :\")\n",
    "print(formatted_dialogues[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Chargement du mod√®le Vigostral-7B-Chat\n",
    "\n",
    "On charge le mod√®le avec :\n",
    "- **Quantization 4-bit** : R√©duit la taille de ~14GB √† ~4GB\n",
    "- **LoRA** : Ajoute des adaptateurs entra√Ænables (~70M params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel_name = \"bofenghuang/vigostral-7b-chat\"\n\n# Configuration de la quantization 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"üì• T√©l√©chargement du mod√®le Vigostral-7B-Chat (peut prendre 5-10 minutes)...\")\n\n# Charger le mod√®le en 4-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Charger le tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(\"‚úÖ Mod√®le charg√© avec succ√®s !\")\nprint(f\"   Taille du mod√®le : {model.get_memory_footprint() / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéõÔ∏è Configuration LoRA\n",
    "\n",
    "LoRA ajoute de petites matrices entra√Ænables aux couches d'attention du mod√®le.\n",
    "\n",
    "**Param√®tres** :\n",
    "- `r=16` : Rang des matrices (plus haut = plus de capacit√©, mais plus de m√©moire)\n",
    "- `lora_alpha=32` : Facteur de scaling\n",
    "- `target_modules` : Couches √† adapter (query et value projections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer le mod√®le pour LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rang des matrices LoRA\n",
    "    lora_alpha=32,  # Facteur de scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Couches √† adapter\n",
    "    lora_dropout=0.05,  # Dropout pour r√©gularisation\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Appliquer LoRA au mod√®le\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher le nombre de param√®tres entra√Ænables\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ LoRA activ√© !\")\n",
    "print(f\"   Param√®tres entra√Ænables : {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "print(f\"   Param√®tres totaux : {all_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Pr√©paration du dataset pour l'entra√Ænement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Cr√©er un dataset HuggingFace\n",
    "dataset_dict = {\"text\": formatted_dialogues}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split train/validation (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"‚úÖ Dataset pr√©par√© :\")\n",
    "print(f\"   Train : {len(dataset['train'])} dialogues\")\n",
    "print(f\"   Validation : {len(dataset['test'])} dialogues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Configuration de l'entra√Ænement\n",
    "\n",
    "**Param√®tres optimis√©s pour T4** :\n",
    "- `per_device_train_batch_size=1` : 1 exemple √† la fois (limite m√©moire)\n",
    "- `gradient_accumulation_steps=4` : Accumule 4 gradients avant mise √† jour (batch effectif = 4)\n",
    "- `num_train_epochs=3` : 3 passages sur le dataset\n",
    "- `learning_rate=2e-4` : Learning rate pour LoRA\n",
    "- `fp16=True` : Utilise float16 pour √©conomiser la m√©moire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vigostral-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",  # Optimizer optimis√© pour m√©moire\n",
    "    warmup_steps=50,\n",
    "    report_to=\"none\",  # D√©sactiver wandb/tensorboard\n",
    ")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configur√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Lancement de l'entra√Ænement\n",
    "\n",
    "Dur√©e estim√©e : **20-30 minutes** sur T4 GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üöÄ D√©but de l'entra√Ænement...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© en {elapsed_time / 60:.1f} minutes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Sauvegarde du mod√®le fine-tun√©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le et les adaptateurs LoRA\n",
    "model.save_pretrained(\"./vigostral-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./vigostral-finetuned-final\")\n",
    "\n",
    "print(\"‚úÖ Mod√®le sauvegard√© dans ./vigostral-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Test du mod√®le fine-tun√©\n",
    "\n",
    "Testons le mod√®le avec quelques prompts pour voir comment il r√©pond dans votre style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse du mod√®le fine-tun√©.\n",
    "    \"\"\"\n",
    "    # Formater le prompt au format Vigostral\n",
    "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    \n",
    "    # Tokenizer\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # G√©n√©rer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # D√©coder la r√©ponse\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraire seulement la r√©ponse (apr√®s [/INST])\n",
    "    if \"[/INST]\" in response:\n",
    "        response = response.split(\"[/INST]\")[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test avec quelques prompts\n",
    "test_prompts = [\n",
    "    \"Explique-moi ton projet principal\",\n",
    "    \"Qu'est-ce que tu penses du machine learning ?\",\n",
    "    \"Comment tu travailles au quotidien ?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Test du mod√®le fine-tun√© :\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nüë§ User: {prompt}\")\n",
    "    response = chat(prompt)\n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí¨ Mode interactif\n",
    "\n",
    "Testez librement votre mod√®le !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí¨ Mode chat interactif\")\n",
    "print(\"Tapez 'quit' pour quitter\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"üë§ Vous: \")\n",
    "    \n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"üëã Au revoir !\")\n",
    "        break\n",
    "    \n",
    "    if not user_input.strip():\n",
    "        continue\n",
    "    \n",
    "    response = chat(user_input)\n",
    "    print(f\"ü§ñ Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ T√©l√©charger le mod√®le fine-tun√©\n",
    "\n",
    "Pour utiliser votre mod√®le localement, t√©l√©chargez les fichiers sauvegard√©s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une archive zip du mod√®le\n",
    "!zip -r vigostral-finetuned-final.zip vigostral-finetuned-final/\n",
    "\n",
    "# T√©l√©charger l'archive\n",
    "from google.colab import files\n",
    "files.download('vigostral-finetuned-final.zip')\n",
    "\n",
    "print(\"‚úÖ Mod√®le t√©l√©charg√© ! Taille des adaptateurs LoRA : ~100-200 MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üéâ F√©licitations !\n\nVous avez fine-tun√© Vigostral-7B-Chat sur votre style personnel !\n\n### üìù Prochaines √©tapes\n\n1. **Tester localement** :\n   ```python\n   from peft import PeftModel\n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   \n   # Charger le mod√®le de base\n   model = AutoModelForCausalLM.from_pretrained(\"bofenghuang/vigostral-7b-chat\")\n   \n   # Charger les adaptateurs LoRA\n   model = PeftModel.from_pretrained(model, \"./vigostral-finetuned-final\")\n   ```\n\n2. **Int√©grer avec nanochat** : Voir documentation s√©par√©e\n\n3. **Am√©liorer le mod√®le** :\n   - Ajouter plus de dialogues (200+ recommand√©)\n   - Augmenter les epochs (5-10)\n   - Ajuster `r` dans LoRA (16 ‚Üí 32 pour plus de capacit√©)\n\n---\n\n**Questions ?** Ouvrez une issue sur GitHub : https://github.com/cladjidane/nanochat-french-tutorial"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}