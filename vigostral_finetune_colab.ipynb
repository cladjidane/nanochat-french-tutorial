{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🇫🇷 Fine-tuning Vigostral-7B-Chat sur votre style personnel\n",
    "\n",
    "Ce notebook vous guide pour fine-tuner le modèle conversationnel français **Vigostral-7B-Chat** sur vos dialogues personnels, en utilisant **LoRA** (Low-Rank Adaptation) pour s'adapter aux contraintes mémoire du GPU T4 gratuit de Google Colab.\n",
    "\n",
    "## 📋 Prérequis\n",
    "\n",
    "1. **GPU T4 activé** : Runtime → Change runtime type → T4 GPU\n",
    "2. **Dataset de dialogues** au format JSONL\n",
    "3. **~30 minutes** de votre temps\n",
    "\n",
    "## 🎯 Objectif\n",
    "\n",
    "Adapter Vigostral-7B-Chat (modèle conversationnel français pré-entraîné sur 213k dialogues) à votre style d'écriture personnel en fine-tunant sur vos propres conversations.\n",
    "\n",
    "## ⚙️ Technique : LoRA (Low-Rank Adaptation)\n",
    "\n",
    "- **Problème** : Fine-tuner 7B paramètres nécessite ~50GB de VRAM (impossible sur T4)\n",
    "- **Solution** : LoRA ne fine-tune que ~1% des paramètres (~70M)\n",
    "- **Avantage** : Tient dans 16GB, training rapide, qualité préservée\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ IMPORTANT : Vérifiez le GPU\n",
    "\n",
    "**Avant de commencer**, assurez-vous que le GPU T4 est activé :\n",
    "1. En haut à droite : **Runtime** → **Change runtime type**\n",
    "2. Sélectionnez **T4 GPU**\n",
    "3. Cliquez **Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que le GPU est disponible\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU détecté : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Mémoire totale : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"❌ ERREUR : Aucun GPU détecté !\")\n",
    "    print(\"   → Allez dans Runtime → Change runtime type → Sélectionnez T4 GPU\")\n",
    "    raise RuntimeError(\"GPU requis pour ce notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 📦 Installation des dépendances\n\nInstallation de toutes les bibliothèques nécessaires. **Durée : 2-3 minutes**\n\n⚠️ **IMPORTANT** : Après l'installation, vous DEVEZ redémarrer le runtime avant de continuer !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"📦 Installation des bibliothèques nécessaires...\")\nprint(\"   (Cela peut prendre 2-3 minutes)\\n\")\n\n# Installer chaque package séparément pour plus de robustesse\n!pip install -q -U transformers\n!pip install -q -U peft\n!pip install -q -U accelerate\n!pip install -q -U bitsandbytes\n!pip install -q -U trl\n\n# ⚠️ IMPORTANT : Forcer pyarrow à une version compatible avec cudf-cu12 de Colab\nprint(\"📌 Installation de pyarrow compatible avec Colab...\")\n!pip install -q 'pyarrow>=14.0.0,<20.0.0'\n\n# Installer datasets 3.x (compatible avec pyarrow < 20.0)\nprint(\"📌 Installation de datasets compatible avec pyarrow < 20.0...\")\n!pip install -q 'datasets>=3.0.0,<4.0.0'\n\nprint(\"\\n✅ Installation terminée !\\n\")\nprint(\"⚠️  ATTENTION : Vous DEVEZ redémarrer le runtime maintenant !\")\nprint(\"   → Runtime → Restart runtime (ou Ctrl+M .)\")\nprint(\"   → Puis re-exécutez la cellule de VÉRIFICATION (ne pas réexécuter cette cellule d'installation)\\n\")\nprint(\"💡 Raison : bitsandbytes nécessite un restart pour charger les extensions CUDA\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ✅ Vérification des bibliothèques (APRÈS RESTART)\n\n**Exécutez cette cellule UNIQUEMENT après avoir redémarré le runtime.**\n\nCette cellule vérifie que toutes les bibliothèques sont correctement installées et que bitsandbytes peut charger ses extensions CUDA.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"🔍 Vérification des bibliothèques...\\n\")\n\n# Vérifier les imports\ntry:\n    import torch\n    print(f\"✅ torch: {torch.__version__}\")\nexcept Exception as e:\n    print(f\"❌ torch: {e}\")\n    raise\n\ntry:\n    import transformers\n    print(f\"✅ transformers: {transformers.__version__}\")\nexcept Exception as e:\n    print(f\"❌ transformers: {e}\")\n    raise\n\ntry:\n    import peft\n    print(f\"✅ peft: {peft.__version__}\")\nexcept Exception as e:\n    print(f\"❌ peft: {e}\")\n    raise\n\ntry:\n    import trl\n    print(f\"✅ trl: {trl.__version__}\")\nexcept Exception as e:\n    print(f\"❌ trl: {e}\")\n    raise\n\ntry:\n    import bitsandbytes\n    print(f\"✅ bitsandbytes: {bitsandbytes.__version__}\")\nexcept Exception as e:\n    print(f\"❌ bitsandbytes: {e}\")\n    raise\n\ntry:\n    import datasets\n    print(f\"✅ datasets: {datasets.__version__}\")\nexcept Exception as e:\n    print(f\"❌ datasets: {e}\")\n    raise\n\ntry:\n    import pyarrow\n    print(f\"✅ pyarrow: {pyarrow.__version__}\")\n    if int(pyarrow.__version__.split('.')[0]) >= 20:\n        print(\"   ⚠️  ATTENTION : pyarrow >= 20 peut causer des problèmes\")\nexcept Exception as e:\n    print(f\"❌ pyarrow: {e}\")\n\n# Test crucial : vérifier que bitsandbytes peut charger ses extensions CUDA\nprint(\"\\n🔍 Test de bitsandbytes avec CUDA...\")\ntry:\n    import bitsandbytes as bnb\n    # Tenter de créer un optimizer (cela force le chargement des extensions CUDA)\n    test_param = torch.nn.Parameter(torch.randn(10, 10).cuda())\n    optimizer = bnb.optim.Adam8bit([test_param], lr=1e-3)\n    print(\"✅ bitsandbytes fonctionne correctement avec CUDA !\")\n    del optimizer, test_param\nexcept Exception as e:\n    print(f\"❌ ERREUR bitsandbytes : {e}\")\n    print(\"\\n⚠️  Vous devez redémarrer le runtime !\")\n    print(\"   → Runtime → Restart runtime\")\n    raise RuntimeError(\"bitsandbytes nécessite un restart du runtime\")\n\nprint(\"\\n✅ Toutes les vérifications sont passées !\")\nprint(\"💡 Vous pouvez maintenant continuer avec le reste du notebook.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📤 Upload de votre dataset\n",
    "\n",
    "Uploadez votre fichier `combined_dataset.jsonl` (format : une ligne par dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"📤 Uploadez votre fichier combined_dataset.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Trouver le fichier uploadé (peut être combined_dataset.jsonl ou combined_dataset (2).jsonl, etc.)\n",
    "dataset_file = None\n",
    "for filename in uploaded.keys():\n",
    "    if 'combined_dataset' in filename and filename.endswith('.jsonl'):\n",
    "        dataset_file = filename\n",
    "        break\n",
    "\n",
    "if dataset_file is None:\n",
    "    raise FileNotFoundError(\"Aucun fichier combined_dataset.jsonl trouvé. Réessayez l'upload.\")\n",
    "\n",
    "# Si le fichier n'est pas exactement \"combined_dataset.jsonl\", le renommer\n",
    "if dataset_file != 'combined_dataset.jsonl':\n",
    "    print(f\"📝 Renommage de '{dataset_file}' en 'combined_dataset.jsonl'\")\n",
    "    shutil.move(dataset_file, 'combined_dataset.jsonl')\n",
    "\n",
    "print(\"✅ Dataset uploadé avec succès !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 Chargement et validation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger le dataset\n",
    "dialogues = []\n",
    "with open('combined_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        dialogues.append(json.loads(line))\n",
    "\n",
    "print(f\"✅ {len(dialogues)} dialogues chargés\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\n📋 Exemple de dialogue :\")\n",
    "print(json.dumps(dialogues[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 Conversion au format Vigostral\n",
    "\n",
    "Vigostral utilise le format Llama-2 : `<s>[INST] question [/INST] réponse </s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue_for_vigostral(dialogue):\n",
    "    \"\"\"\n",
    "    Convertit un dialogue du format OpenAI au format Vigostral.\n",
    "    \n",
    "    Vigostral utilise le format Llama-2 :\n",
    "    <s>[INST] question [/INST] réponse </s>\n",
    "    \"\"\"\n",
    "    messages = dialogue['messages']\n",
    "    formatted_text = \"<s>\"\n",
    "    \n",
    "    for i in range(0, len(messages), 2):\n",
    "        if i < len(messages) and messages[i]['role'] == 'user':\n",
    "            user_msg = messages[i]['content']\n",
    "            formatted_text += f\"[INST] {user_msg} [/INST]\"\n",
    "        \n",
    "        if i + 1 < len(messages) and messages[i + 1]['role'] == 'assistant':\n",
    "            assistant_msg = messages[i + 1]['content']\n",
    "            formatted_text += f\" {assistant_msg} </s>\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Convertir tous les dialogues\n",
    "formatted_dialogues = [format_dialogue_for_vigostral(d) for d in dialogues]\n",
    "\n",
    "print(f\"✅ {len(formatted_dialogues)} dialogues formatés\")\n",
    "print(\"\\n📋 Exemple de dialogue formaté :\")\n",
    "print(formatted_dialogues[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🤖 Chargement du modèle Vigostral-7B-Chat\n",
    "\n",
    "On charge le modèle avec :\n",
    "- **Quantization 4-bit** : Réduit la taille de ~14GB à ~4GB\n",
    "- **LoRA** : Ajoute des adaptateurs entraînables (~70M params)\n",
    "\n",
    "**⏳ Durée estimée : 5-10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ✅ Nom correct du modèle\n",
    "model_name = \"bofenghuang/vigostral-7b-chat\"\n",
    "\n",
    "# Configuration de la quantization 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"📥 Téléchargement du modèle Vigostral-7B-Chat...\")\n",
    "print(\"   (Peut prendre 5-10 minutes la première fois)\\n\")\n",
    "\n",
    "# Charger le modèle en 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"✅ Modèle chargé avec succès !\")\n",
    "print(f\"   Taille du modèle : {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎛️ Configuration LoRA\n",
    "\n",
    "LoRA ajoute de petites matrices entraînables aux couches d'attention du modèle.\n",
    "\n",
    "**Paramètres** :\n",
    "- `r=16` : Rang des matrices (plus haut = plus de capacité, mais plus de mémoire)\n",
    "- `lora_alpha=32` : Facteur de scaling\n",
    "- `target_modules` : Couches à adapter (query et value projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer le modèle pour LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rang des matrices LoRA\n",
    "    lora_alpha=32,  # Facteur de scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Couches à adapter\n",
    "    lora_dropout=0.05,  # Dropout pour régularisation\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Appliquer LoRA au modèle\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher le nombre de paramètres entraînables\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✅ LoRA activé !\")\n",
    "print(f\"   Paramètres entraînables : {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "print(f\"   Paramètres totaux : {all_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 Préparation du dataset pour l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Créer un dataset HuggingFace\n",
    "dataset_dict = {\"text\": formatted_dialogues}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split train/validation (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"✅ Dataset préparé :\")\n",
    "print(f\"   Train : {len(dataset['train'])} dialogues\")\n",
    "print(f\"   Validation : {len(dataset['test'])} dialogues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 Configuration de l'entraînement\n",
    "\n",
    "**Paramètres optimisés pour T4** :\n",
    "- `num_train_epochs=3` : 3 passages sur le dataset\n",
    "- `per_device_train_batch_size=1` : 1 exemple à la fois (limite mémoire)\n",
    "- `gradient_accumulation_steps=4` : Accumule 4 gradients avant mise à jour (batch effectif = 4)\n",
    "- `learning_rate=2e-4` : Learning rate pour LoRA\n",
    "- `save_strategy=\"steps\"` : Sauvegarde tous les 50 steps (évite de perdre le travail en cas de crash)\n",
    "- `fp16=True` : Utilise float16 pour économiser la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ✅ Configuration avec SFTConfig (pas TrainingArguments)\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./vigostral-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"steps\",      # ✅ Sauvegarder tous les N steps\n",
    "    save_steps=50,               # ✅ Tous les 50 steps (~3-5 min)\n",
    "    logging_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",    # Optimizer optimisé pour mémoire\n",
    "    warmup_steps=50,\n",
    "    report_to=\"none\",            # Désactiver wandb/tensorboard\n",
    "    dataset_text_field=\"text\",   # ✅ Champ contenant le texte\n",
    "    packing=False,               # ✅ Désactiver le packing\n",
    "    max_length=512,              # ✅ Longueur max (pas max_seq_length !)\n",
    ")\n",
    "\n",
    "# Créer le trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer configuré !\")\n",
    "print(f\"   Nombre total de steps : ~{len(dataset['train']) * 3 // 4}\")\n",
    "print(f\"   Checkpoints seront sauvegardés tous les 50 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Lancement de l'entraînement\n",
    "\n",
    "**⏳ Durée estimée : 20-30 minutes sur T4 GPU**\n",
    "\n",
    "⚠️ **IMPORTANT** : Gardez cet onglet ouvert et visible pendant l'entraînement pour éviter la déconnexion !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"🚀 Début de l'entraînement...\\n\")\n",
    "print(\"⚠️  Gardez cet onglet ouvert pendant le training !\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Lancer le training\n",
    "trainer.train()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n✅ Entraînement terminé en {elapsed_time / 60:.1f} minutes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💾 Sauvegarde du modèle fine-tuné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle et les adaptateurs LoRA\n",
    "model.save_pretrained(\"./vigostral-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./vigostral-finetuned-final\")\n",
    "\n",
    "print(\"✅ Modèle sauvegardé dans ./vigostral-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧪 Test du modèle fine-tuné\n",
    "\n",
    "Testons le modèle avec quelques prompts pour voir comment il répond dans votre style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Génère une réponse du modèle fine-tuné.\n",
    "    \"\"\"\n",
    "    # Formater le prompt au format Vigostral\n",
    "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    \n",
    "    # Tokenizer\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Générer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Décoder la réponse\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraire seulement la réponse (après [/INST])\n",
    "    if \"[/INST]\" in response:\n",
    "        response = response.split(\"[/INST]\")[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test avec quelques prompts\n",
    "test_prompts = [\n",
    "    \"Explique-moi ton projet principal\",\n",
    "    \"Qu'est-ce que tu penses du machine learning ?\",\n",
    "    \"Comment tu travailles au quotidien ?\",\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 Test du modèle fine-tuné :\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n👤 User: {prompt}\")\n",
    "    response = chat(prompt)\n",
    "    print(f\"🤖 Assistant: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💬 Mode interactif\n",
    "\n",
    "Testez librement votre modèle !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💬 Mode chat interactif\")\n",
    "print(\"Tapez 'quit' pour quitter\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"👤 Vous: \")\n",
    "    \n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"👋 Au revoir !\")\n",
    "        break\n",
    "    \n",
    "    if not user_input.strip():\n",
    "        continue\n",
    "    \n",
    "    response = chat(user_input)\n",
    "    print(f\"🤖 Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📦 Télécharger le modèle fine-tuné\n",
    "\n",
    "Pour utiliser votre modèle localement, téléchargez les fichiers sauvegardés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une archive zip du modèle\n",
    "!zip -r vigostral-finetuned-final.zip vigostral-finetuned-final/\n",
    "\n",
    "# Télécharger l'archive\n",
    "from google.colab import files\n",
    "files.download('vigostral-finetuned-final.zip')\n",
    "\n",
    "print(\"✅ Modèle téléchargé ! Taille des adaptateurs LoRA : ~100-200 MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 Félicitations !\n",
    "\n",
    "Vous avez fine-tuné Vigostral-7B-Chat sur votre style personnel !\n",
    "\n",
    "### 📝 Prochaines étapes\n",
    "\n",
    "1. **Tester localement** :\n",
    "   ```bash\n",
    "   # Dézippez le modèle\n",
    "   unzip vigostral-finetuned-final.zip\n",
    "   \n",
    "   # Lancez l'interface Gradio\n",
    "   python chat_gradio.py\n",
    "   ```\n",
    "\n",
    "2. **Améliorer le modèle** :\n",
    "   - Ajouter plus de dialogues (200+ recommandé)\n",
    "   - Augmenter les epochs (`num_train_epochs=5`)\n",
    "   - Ajuster `r` dans LoRA (16 → 32 pour plus de capacité)\n",
    "\n",
    "3. **Intégrer avec nanochat** : Voir `INTEGRATION_NANOCHAT.md`\n",
    "\n",
    "---\n",
    "\n",
    "**Questions ?** Ouvrez une issue sur GitHub : https://github.com/cladjidane/nanochat-french-tutorial/issues\n",
    "\n",
    "**Made with ❤️ for the French AI community**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}