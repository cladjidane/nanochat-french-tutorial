{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üá´üá∑ Fine-tuning Vigostral-7B-Chat sur votre style personnel\n",
    "\n",
    "Ce notebook vous guide pour fine-tuner le mod√®le conversationnel fran√ßais **Vigostral-7B-Chat** sur vos dialogues personnels, en utilisant **LoRA** (Low-Rank Adaptation) pour s'adapter aux contraintes m√©moire du GPU T4 gratuit de Google Colab.\n",
    "\n",
    "## üìã Pr√©requis\n",
    "\n",
    "1. **GPU T4 activ√©** : Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "2. **Dataset de dialogues** au format JSONL\n",
    "3. **~30 minutes** de votre temps\n",
    "\n",
    "## üéØ Objectif\n",
    "\n",
    "Adapter Vigostral-7B-Chat (mod√®le conversationnel fran√ßais pr√©-entra√Æn√© sur 213k dialogues) √† votre style d'√©criture personnel en fine-tunant sur vos propres conversations.\n",
    "\n",
    "## ‚öôÔ∏è Technique : LoRA (Low-Rank Adaptation)\n",
    "\n",
    "- **Probl√®me** : Fine-tuner 7B param√®tres n√©cessite ~50GB de VRAM (impossible sur T4)\n",
    "- **Solution** : LoRA ne fine-tune que ~1% des param√®tres (~70M)\n",
    "- **Avantage** : Tient dans 16GB, training rapide, qualit√© pr√©serv√©e\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANT : V√©rifiez le GPU\n",
    "\n",
    "**Avant de commencer**, assurez-vous que le GPU T4 est activ√© :\n",
    "1. En haut √† droite : **Runtime** ‚Üí **Change runtime type**\n",
    "2. S√©lectionnez **T4 GPU**\n",
    "3. Cliquez **Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier que le GPU est disponible\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU d√©tect√© : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   M√©moire totale : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå ERREUR : Aucun GPU d√©tect√© !\")\n",
    "    print(\"   ‚Üí Allez dans Runtime ‚Üí Change runtime type ‚Üí S√©lectionnez T4 GPU\")\n",
    "    raise RuntimeError(\"GPU requis pour ce notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üì¶ Installation des d√©pendances\n\nInstallation de toutes les biblioth√®ques n√©cessaires. **Dur√©e : 2-3 minutes**\n\n‚ö†Ô∏è **IMPORTANT** : Apr√®s l'installation, vous DEVEZ red√©marrer le runtime avant de continuer !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üì¶ Installation des biblioth√®ques n√©cessaires...\")\nprint(\"   (Cela peut prendre 2-3 minutes)\\n\")\n\n# Installer chaque package s√©par√©ment pour plus de robustesse\n!pip install -q -U transformers\n!pip install -q -U peft\n!pip install -q -U accelerate\n!pip install -q -U bitsandbytes\n!pip install -q -U trl\n\n# ‚ö†Ô∏è IMPORTANT : Forcer pyarrow √† une version compatible avec cudf-cu12 de Colab\nprint(\"üìå Installation de pyarrow compatible avec Colab...\")\n!pip install -q 'pyarrow>=14.0.0,<20.0.0'\n\n# Installer datasets 3.x (compatible avec pyarrow < 20.0)\nprint(\"üìå Installation de datasets compatible avec pyarrow < 20.0...\")\n!pip install -q 'datasets>=3.0.0,<4.0.0'\n\nprint(\"\\n‚úÖ Installation termin√©e !\\n\")\nprint(\"‚ö†Ô∏è  ATTENTION : Vous DEVEZ red√©marrer le runtime maintenant !\")\nprint(\"   ‚Üí Runtime ‚Üí Restart runtime (ou Ctrl+M .)\")\nprint(\"   ‚Üí Puis re-ex√©cutez la cellule de V√âRIFICATION (ne pas r√©ex√©cuter cette cellule d'installation)\\n\")\nprint(\"üí° Raison : bitsandbytes n√©cessite un restart pour charger les extensions CUDA\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ‚úÖ V√©rification des biblioth√®ques (APR√àS RESTART)\n\n**Ex√©cutez cette cellule UNIQUEMENT apr√®s avoir red√©marr√© le runtime.**\n\nCette cellule v√©rifie que toutes les biblioth√®ques sont correctement install√©es et que bitsandbytes peut charger ses extensions CUDA.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"üîç V√©rification des biblioth√®ques...\\n\")\n\n# V√©rifier les imports\ntry:\n    import torch\n    print(f\"‚úÖ torch: {torch.__version__}\")\nexcept Exception as e:\n    print(f\"‚ùå torch: {e}\")\n    raise\n\ntry:\n    import transformers\n    print(f\"‚úÖ transformers: {transformers.__version__}\")\nexcept Exception as e:\n    print(f\"‚ùå transformers: {e}\")\n    raise\n\ntry:\n    import peft\n    print(f\"‚úÖ peft: {peft.__version__}\")\nexcept Exception as e:\n    print(f\"‚ùå peft: {e}\")\n    raise\n\ntry:\n    import trl\n    print(f\"‚úÖ trl: {trl.__version__}\")\nexcept Exception as e:\n    print(f\"‚ùå trl: {e}\")\n    raise\n\ntry:\n    import bitsandbytes\n    print(f\"‚úÖ bitsandbytes: {bitsandbytes.__version__}\")\nexcept Exception as e:\n    print(f\"‚ùå bitsandbytes: {e}\")\n    raise\n\ntry:\n    import datasets\n    print(f\"‚úÖ datasets: {datasets.__version__}\")\nexcept Exception as e:\n    print(f\"‚ùå datasets: {e}\")\n    raise\n\ntry:\n    import pyarrow\n    print(f\"‚úÖ pyarrow: {pyarrow.__version__}\")\n    if int(pyarrow.__version__.split('.')[0]) >= 20:\n        print(\"   ‚ö†Ô∏è  ATTENTION : pyarrow >= 20 peut causer des probl√®mes\")\nexcept Exception as e:\n    print(f\"‚ùå pyarrow: {e}\")\n\n# Test crucial : v√©rifier que bitsandbytes peut charger ses extensions CUDA\nprint(\"\\nüîç Test de bitsandbytes avec CUDA...\")\ntry:\n    import bitsandbytes as bnb\n    # Tenter de cr√©er un optimizer (cela force le chargement des extensions CUDA)\n    test_param = torch.nn.Parameter(torch.randn(10, 10).cuda())\n    optimizer = bnb.optim.Adam8bit([test_param], lr=1e-3)\n    print(\"‚úÖ bitsandbytes fonctionne correctement avec CUDA !\")\n    del optimizer, test_param\nexcept Exception as e:\n    print(f\"‚ùå ERREUR bitsandbytes : {e}\")\n    print(\"\\n‚ö†Ô∏è  Vous devez red√©marrer le runtime !\")\n    print(\"   ‚Üí Runtime ‚Üí Restart runtime\")\n    raise RuntimeError(\"bitsandbytes n√©cessite un restart du runtime\")\n\nprint(\"\\n‚úÖ Toutes les v√©rifications sont pass√©es !\")\nprint(\"üí° Vous pouvez maintenant continuer avec le reste du notebook.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì§ Upload de votre dataset\n",
    "\n",
    "Uploadez votre fichier `combined_dataset.jsonl` (format : une ligne par dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"üì§ Uploadez votre fichier combined_dataset.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Trouver le fichier upload√© (peut √™tre combined_dataset.jsonl ou combined_dataset (2).jsonl, etc.)\n",
    "dataset_file = None\n",
    "for filename in uploaded.keys():\n",
    "    if 'combined_dataset' in filename and filename.endswith('.jsonl'):\n",
    "        dataset_file = filename\n",
    "        break\n",
    "\n",
    "if dataset_file is None:\n",
    "    raise FileNotFoundError(\"Aucun fichier combined_dataset.jsonl trouv√©. R√©essayez l'upload.\")\n",
    "\n",
    "# Si le fichier n'est pas exactement \"combined_dataset.jsonl\", le renommer\n",
    "if dataset_file != 'combined_dataset.jsonl':\n",
    "    print(f\"üìù Renommage de '{dataset_file}' en 'combined_dataset.jsonl'\")\n",
    "    shutil.move(dataset_file, 'combined_dataset.jsonl')\n",
    "\n",
    "print(\"‚úÖ Dataset upload√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Chargement et validation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger le dataset\n",
    "dialogues = []\n",
    "with open('combined_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        dialogues.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úÖ {len(dialogues)} dialogues charg√©s\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\nüìã Exemple de dialogue :\")\n",
    "print(json.dumps(dialogues[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Conversion au format Vigostral\n",
    "\n",
    "Vigostral utilise le format Llama-2 : `<s>[INST] question [/INST] r√©ponse </s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue_for_vigostral(dialogue):\n",
    "    \"\"\"\n",
    "    Convertit un dialogue du format OpenAI au format Vigostral.\n",
    "    \n",
    "    Vigostral utilise le format Llama-2 :\n",
    "    <s>[INST] question [/INST] r√©ponse </s>\n",
    "    \"\"\"\n",
    "    messages = dialogue['messages']\n",
    "    formatted_text = \"<s>\"\n",
    "    \n",
    "    for i in range(0, len(messages), 2):\n",
    "        if i < len(messages) and messages[i]['role'] == 'user':\n",
    "            user_msg = messages[i]['content']\n",
    "            formatted_text += f\"[INST] {user_msg} [/INST]\"\n",
    "        \n",
    "        if i + 1 < len(messages) and messages[i + 1]['role'] == 'assistant':\n",
    "            assistant_msg = messages[i + 1]['content']\n",
    "            formatted_text += f\" {assistant_msg} </s>\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Convertir tous les dialogues\n",
    "formatted_dialogues = [format_dialogue_for_vigostral(d) for d in dialogues]\n",
    "\n",
    "print(f\"‚úÖ {len(formatted_dialogues)} dialogues format√©s\")\n",
    "print(\"\\nüìã Exemple de dialogue format√© :\")\n",
    "print(formatted_dialogues[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Chargement du mod√®le Vigostral-7B-Chat\n",
    "\n",
    "On charge le mod√®le avec :\n",
    "- **Quantization 4-bit** : R√©duit la taille de ~14GB √† ~4GB\n",
    "- **LoRA** : Ajoute des adaptateurs entra√Ænables (~70M params)\n",
    "\n",
    "**‚è≥ Dur√©e estim√©e : 5-10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ‚úÖ Nom correct du mod√®le\n",
    "model_name = \"bofenghuang/vigostral-7b-chat\"\n",
    "\n",
    "# Configuration de la quantization 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"üì• T√©l√©chargement du mod√®le Vigostral-7B-Chat...\")\n",
    "print(\"   (Peut prendre 5-10 minutes la premi√®re fois)\\n\")\n",
    "\n",
    "# Charger le mod√®le en 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s !\")\n",
    "print(f\"   Taille du mod√®le : {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéõÔ∏è Configuration LoRA\n",
    "\n",
    "LoRA ajoute de petites matrices entra√Ænables aux couches d'attention du mod√®le.\n",
    "\n",
    "**Param√®tres** :\n",
    "- `r=16` : Rang des matrices (plus haut = plus de capacit√©, mais plus de m√©moire)\n",
    "- `lora_alpha=32` : Facteur de scaling\n",
    "- `target_modules` : Couches √† adapter (query et value projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer le mod√®le pour LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rang des matrices LoRA\n",
    "    lora_alpha=32,  # Facteur de scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Couches √† adapter\n",
    "    lora_dropout=0.05,  # Dropout pour r√©gularisation\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Appliquer LoRA au mod√®le\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher le nombre de param√®tres entra√Ænables\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ LoRA activ√© !\")\n",
    "print(f\"   Param√®tres entra√Ænables : {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "print(f\"   Param√®tres totaux : {all_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Pr√©paration du dataset pour l'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Cr√©er un dataset HuggingFace\n",
    "dataset_dict = {\"text\": formatted_dialogues}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split train/validation (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"‚úÖ Dataset pr√©par√© :\")\n",
    "print(f\"   Train : {len(dataset['train'])} dialogues\")\n",
    "print(f\"   Validation : {len(dataset['test'])} dialogues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Configuration de l'entra√Ænement\n",
    "\n",
    "**Param√®tres optimis√©s pour T4** :\n",
    "- `num_train_epochs=3` : 3 passages sur le dataset\n",
    "- `per_device_train_batch_size=1` : 1 exemple √† la fois (limite m√©moire)\n",
    "- `gradient_accumulation_steps=4` : Accumule 4 gradients avant mise √† jour (batch effectif = 4)\n",
    "- `learning_rate=2e-4` : Learning rate pour LoRA\n",
    "- `save_strategy=\"steps\"` : Sauvegarde tous les 50 steps (√©vite de perdre le travail en cas de crash)\n",
    "- `fp16=True` : Utilise float16 pour √©conomiser la m√©moire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ‚úÖ Configuration avec SFTConfig (pas TrainingArguments)\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./vigostral-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"steps\",      # ‚úÖ Sauvegarder tous les N steps\n",
    "    save_steps=50,               # ‚úÖ Tous les 50 steps (~3-5 min)\n",
    "    logging_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",    # Optimizer optimis√© pour m√©moire\n",
    "    warmup_steps=50,\n",
    "    report_to=\"none\",            # D√©sactiver wandb/tensorboard\n",
    "    dataset_text_field=\"text\",   # ‚úÖ Champ contenant le texte\n",
    "    packing=False,               # ‚úÖ D√©sactiver le packing\n",
    "    max_length=512,              # ‚úÖ Longueur max (pas max_seq_length !)\n",
    ")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configur√© !\")\n",
    "print(f\"   Nombre total de steps : ~{len(dataset['train']) * 3 // 4}\")\n",
    "print(f\"   Checkpoints seront sauvegard√©s tous les 50 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Lancement de l'entra√Ænement\n",
    "\n",
    "**‚è≥ Dur√©e estim√©e : 20-30 minutes sur T4 GPU**\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT** : Gardez cet onglet ouvert et visible pendant l'entra√Ænement pour √©viter la d√©connexion !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üöÄ D√©but de l'entra√Ænement...\\n\")\n",
    "print(\"‚ö†Ô∏è  Gardez cet onglet ouvert pendant le training !\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Lancer le training\n",
    "trainer.train()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© en {elapsed_time / 60:.1f} minutes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Sauvegarde du mod√®le fine-tun√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le et les adaptateurs LoRA\n",
    "model.save_pretrained(\"./vigostral-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./vigostral-finetuned-final\")\n",
    "\n",
    "print(\"‚úÖ Mod√®le sauvegard√© dans ./vigostral-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Test du mod√®le fine-tun√©\n",
    "\n",
    "Testons le mod√®le avec quelques prompts pour voir comment il r√©pond dans votre style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse du mod√®le fine-tun√©.\n",
    "    \"\"\"\n",
    "    # Formater le prompt au format Vigostral\n",
    "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    \n",
    "    # Tokenizer\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # G√©n√©rer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # D√©coder la r√©ponse\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraire seulement la r√©ponse (apr√®s [/INST])\n",
    "    if \"[/INST]\" in response:\n",
    "        response = response.split(\"[/INST]\")[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test avec quelques prompts\n",
    "test_prompts = [\n",
    "    \"Explique-moi ton projet principal\",\n",
    "    \"Qu'est-ce que tu penses du machine learning ?\",\n",
    "    \"Comment tu travailles au quotidien ?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Test du mod√®le fine-tun√© :\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nüë§ User: {prompt}\")\n",
    "    response = chat(prompt)\n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí¨ Mode interactif\n",
    "\n",
    "Testez librement votre mod√®le !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí¨ Mode chat interactif\")\n",
    "print(\"Tapez 'quit' pour quitter\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"üë§ Vous: \")\n",
    "    \n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"üëã Au revoir !\")\n",
    "        break\n",
    "    \n",
    "    if not user_input.strip():\n",
    "        continue\n",
    "    \n",
    "    response = chat(user_input)\n",
    "    print(f\"ü§ñ Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ T√©l√©charger le mod√®le fine-tun√©\n",
    "\n",
    "Pour utiliser votre mod√®le localement, t√©l√©chargez les fichiers sauvegard√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une archive zip du mod√®le\n",
    "!zip -r vigostral-finetuned-final.zip vigostral-finetuned-final/\n",
    "\n",
    "# T√©l√©charger l'archive\n",
    "from google.colab import files\n",
    "files.download('vigostral-finetuned-final.zip')\n",
    "\n",
    "print(\"‚úÖ Mod√®le t√©l√©charg√© ! Taille des adaptateurs LoRA : ~100-200 MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ F√©licitations !\n",
    "\n",
    "Vous avez fine-tun√© Vigostral-7B-Chat sur votre style personnel !\n",
    "\n",
    "### üìù Prochaines √©tapes\n",
    "\n",
    "1. **Tester localement** :\n",
    "   ```bash\n",
    "   # D√©zippez le mod√®le\n",
    "   unzip vigostral-finetuned-final.zip\n",
    "   \n",
    "   # Lancez l'interface Gradio\n",
    "   python chat_gradio.py\n",
    "   ```\n",
    "\n",
    "2. **Am√©liorer le mod√®le** :\n",
    "   - Ajouter plus de dialogues (200+ recommand√©)\n",
    "   - Augmenter les epochs (`num_train_epochs=5`)\n",
    "   - Ajuster `r` dans LoRA (16 ‚Üí 32 pour plus de capacit√©)\n",
    "\n",
    "3. **Int√©grer avec nanochat** : Voir `INTEGRATION_NANOCHAT.md`\n",
    "\n",
    "---\n",
    "\n",
    "**Questions ?** Ouvrez une issue sur GitHub : https://github.com/cladjidane/nanochat-french-tutorial/issues\n",
    "\n",
    "**Made with ‚ù§Ô∏è for the French AI community**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}